{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc5659e2-76af-4188-8f0e-15ecb9fd553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "from transformers import AutoTokenizer, BatchEncoding, AutoModel, EvalPrediction, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.utils import ModelOutput\n",
    "from scipy.stats import spearmanr\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02b451fa-d1a9-4db5-bb0b-74442da1b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39a02884-b2eb-48e3-a41e-b2d26ac5a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数の固定\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ba183c8-fb41-40d4-bcb5-92b045078fab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c467062a11440ca027a983a8db56f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c3e4d8478a46ac991574373054e664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/24387500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 24387500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# データのロード\n",
    "unsup_train_dataset = load_dataset('llm-book/jawiki-sentences', split=\"train\")\n",
    "\n",
    "print(unsup_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440dd4b6-525a-457b-abce-32376bb8b600",
   "metadata": {},
   "source": [
    "---\n",
    "＜ポイント＞  \n",
    "・textの列しかもっていない   \n",
    "・2千万のデータ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0725dd4a-ee5a-44f9-a476-22b65965a5de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 アンパサンド(&, 英語: ampersand)は、並立助詞「...と...」を意味する記号である。\n",
      "1 ラテン語で「...と...」を表す接続詞 \"et\" の合字を起源とする。\n",
      "2 現代のフォントでも、Trebuchet MS など一部のフォントでは、\"et\" の合字であることが容易にわかる字形を使用している。\n",
      "3 英語で教育を行う学校でアルファベットを復唱する場合、その文字自体が単語となる文字(\"A\", \"I\", かつては \"O\" も)については、伝統的にラテン語の per se(それ自体)を用いて \"A per se A\" のように唱えられていた。\n",
      "4 また、アルファベットの最後に、27番目の文字のように \"&\" を加えることも広く行われていた。\n",
      "5 \"&\" はラテン語で et と読まれていたが、後に英語で and と読まれるようになった。\n",
      "6 結果として、アルファベットの復唱の最後は \"X, Y, Z, and per se and\" という形になった。\n",
      "7 この最後のフレーズが繰り返されるうちに \"ampersand\" と訛っていき、この言葉は1837年までには英語の一般的な語法となった。\n",
      "8 アンドレ=マリ・アンペールがこの記号を自身の著作で使い、これが広く読まれたため、この記号が \"Ampère's and\" と呼ばれるようになったという誤った語源俗説がある。\n",
      "9 アンパサンドの起源は1世紀の古ローマ筆記体にまで遡ることができる。\n",
      "10 古ローマ筆記体では、E と T はしばしば合字として繋げて書かれていた(左図「アンパサンドの変遷」の字形1)。それに続く、流麗さを増した新ローマ筆記体では、様々な合字が極めて頻繁に使われるようになった。\n",
      "11 字形2と3は4世紀中頃における et の合字の例である。\n",
      "12 その後、9世紀のカロリング小文字体に至るラテン文字の変遷の過程で、合字の使用は一般には廃れていった。\n",
      "13 しかし、et の合字は使われ続け、次第に元の文字がわかりにくい字形に変化していった(字形4から6)。\n",
      "14 現代のイタリック体のアンパサンドは、ルネサンス期に発展した筆記体での et の合字に遡る。\n",
      "15 1455年のヨーロッパにおける印刷技術の発明以降、印刷業者はイタリック体とローマ筆記体のアンパサンドの両方を多用するようになった。\n",
      "16 アンパサンドのルーツはローマ時代に遡るため、ラテンアルファベットを使用する多くの言語でアンパサンドが使用されるようになった。\n",
      "17 アンパサンドはしばしばラテンアルファベットの最後の文字とされることがあった。\n",
      "18 例えば1011年のByrhtferthの文字表がその例である。\n",
      "19 同様に、\"&\" は英語アルファベットの27番目の文字とされ、アメリカ合衆国やその他の地域でも、子供達はアンパサンドはアルファベットの最後の文字だと教えられていた。\n",
      "20 1863年の M. B. Moore の著書 The Dixie Primer, for the Little Folks にその一例を見ることができる。\n",
      "21 ジョージ・エリオットは、1859年に発表した小説「アダム・ビード(英語版)」の中で、Jacob Storey に次のセリフを語らせている。\n",
      "22 \"He thought it [Z] had only been put to finish off th' alphabet like; though ampusand would ha' done as well, for what he could see.\" よく知られた童謡の Apple Pie ABC は \"X, Y, Z, and ampersand, All wished for a piece in hand\" という歌詞で締めくくられる。\n",
      "23 アンパサンドは、ティロ式記号の et (\"⁊\", Unicode U+204A) とは別のものである。\n",
      "24 ティロ式記号の et は、アンパサンドと意味は同じだが数字の「7」に似た形の記号である。\n",
      "25 両者はともに古代から使用され、中世を通してラテン語の et を表すために使用された。\n",
      "26 しかし、アンパサンドとティロ式記号の et はそれぞれ独立に発明されたものである。\n",
      "27 ラテン文字から発展した古アイルランド語の文字では、アイルランド語の agus(「...と...」)を表すためにティロ式記号の et が使用されていた。\n",
      "28 今日はゲール文字の一部として主に装飾的な目的で使用されている。\n",
      "29 この文字はアイルランドにおけるキリスト教時代初期に修道院の影響によって書き文字に加わった可能性がある。\n",
      "30 日常的な手書きの場合、欧米では小文字の ε(エプシロン)を大きくしたもの(あるいは数字の \"3\" の鏡文字)に縦線を加えた形の単純化されたアンパサンドがしばしば使われる。\n",
      "31 また、エプシロンの上下に縦線または点を付けたものもしばしば使われる。\n",
      "32 くだけた用法として、プラス記号(\"+\", この記号もまた et の合字である)がアンパサンドの代わりに使われることもある。\n",
      "33 また、プラス記号に輪を重ねたような、無声歯茎側面摩擦音を示す発音記号「[ɬ]」のようなものが使われることもある。\n",
      "34 ティロの速記には「et」を表すための「⁊」(U+204A Tironian sign et)がある。\n",
      "35 この文字はドイツのフラクトゥールで使われたほか、ゲール文字でも使用される。\n",
      "36 ギリシア文字では「......と」を意味するκαιを表すための合字として「ϗ」(U+03D7 Greek kai symbol)が使われることがある。\n",
      "37 プログラミング言語では、C など多数の言語で AND 演算子として用いられる。\n",
      "38 PHPでは、変数宣言記号($)の直前に記述することで、参照渡しを行うことができる。\n",
      "39 BASIC 系列の言語では文字列の連結演算子として使用される。\n",
      "40 \"foo\" & \"bar\" は \"foobar\" を返す。\n",
      "41 また、主にマイクロソフト系では整数の十六進表記に &h を用い、&h0F (十進で15)のように表現する。\n",
      "42 SGML、XML、HTMLでは、アンパサンドを使ってSGML実体を参照する。\n",
      "43 \n",
      "44 言語(げんご)は、狭義には「声による記号の体系」をいう。\n",
      "45 広辞苑や大辞泉には次のように解説されている。\n",
      "46 『日本大百科全書』では、「言語」という語は多義である、と解説され、大脳の言語中枢(英語版)に蓄えられた《語彙と文法規則の体系》を指すこともあり、その体系を用いる能力としてとらえることもある、と解説され、一方では、抽象的に「すべての人間が共有する言語能力」を指すこともあり、「個々の個別言語」を指すこともある、と解説されている。\n",
      "47 広義の言語には、verbalなものとnon-verbalなもの(各種記号、アイコン、図形、ボディーランゲージ等)の両方を含み、日常のコミュニケーションでは狭義の言語表現に身振り、手振り、図示、擬音等も加えて表現されることもある。\n",
      "48 言語は、人間が用いる意志伝達手段であり、社会集団内で形成習得され、意志を相互に伝達することや、抽象的な思考を可能にし、結果として人間の社会的活動や文化的活動を支えている。\n",
      "49 言語には、文化の特徴が織り込まれており、共同体で用いられている言語の習得をすることによって、その共同体での社会的学習、および人格の形成をしていくことになる。\n"
     ]
    }
   ],
   "source": [
    "# 最初の50件を表示\n",
    "for i, text in enumerate(unsup_train_dataset[:50]['text']):\n",
    "    print(i, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a525ba31-f484-4525-a4fb-e3a093e1a997",
   "metadata": {},
   "source": [
    "---\n",
    "＜ポイント＞  \n",
    "・空白の行があるので、これを前処理で除く必要がある。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8705075d-a2e0-4613-a47f-cc4db13a0045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e2afa207b1451db56cd7452b87ac78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/23048277 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 空行を削除\n",
    "unsup_train_dataset = unsup_train_dataset.filter(lambda example: example['text'].strip() != \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06f344b6-5f01-4e5d-ab5a-f5b6722b7e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['また、主にマイクロソフト系では整数の十六進表記に &h を用い、&h0F (十進で15)のように表現する。',\n",
       "  'SGML、XML、HTMLでは、アンパサンドを使ってSGML実体を参照する。',\n",
       "  '言語(げんご)は、狭義には「声による記号の体系」をいう。',\n",
       "  '広辞苑や大辞泉には次のように解説されている。']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsup_train_dataset[41:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96844614-36a4-40d2-9bc1-30d8f949dae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15f5bea0c02416aa5d1c1b65a67219d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 100万データをランダムサンプリング\n",
    "unsup_train_dataset = unsup_train_dataset.shuffle().select(range(1000000))\n",
    "\n",
    "# 参照によるマッピングではなく、シャッフルしてサンプリングされたデータセットをディスクに書き込む\n",
    "unsup_train_dataset = unsup_train_dataset.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dae5ab7a-e2fc-471f-9ff0-99a3f9a2728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000000\n",
      "})\n",
      "\n",
      "==================================================\n",
      "\n",
      "0 その学校は基本モード手法からアヴァンギャルド系のジャズ学校でリディアン・クロマティック・コンセプトの大家ジョージ・ラッセル(英語版)も時折訪れて指導をしていた。\n",
      "1 延暦5年(786年)内舎人となったが、3年後の延暦8年(789年)に史都蒙の予言通り、32歳で病により自邸で没した。\n",
      "2 「あ号作戦後の兵装増備状況調査」によるとあ号作戦(1944年6月)時点での各艦の対空機銃は以下の通りとされている。\n",
      "3 一方、5月に領土の返還を求めるフランスがタイ領を攻撃し、国際社会への復帰を優先せざるを得ないタイは、1941年に併合した領土の引き渡しに応じ、ナコーン・チャンパーサック県(英語版)(チャンパーサック州)、ピブーンソンクラーム県(英語版)(シェムリアップ州)、プレアタボン県(英語版)(バタンバン州)の3県がフランスに返還された。\n",
      "4 およそ140万台が生産された。\n",
      "5 また、海兵寮同期であった山本とは当時からあまりウマが合わず、4歳年長であった日高がしばしば山本を軽んじる態度を見せたためともいわれている。\n",
      "6 これら初期の探検家や入植者の中で最も有名な者がダニエル・ブーンであり、伝統的にケンタッキー州創設者の一人と考えられている。\n",
      "7 全米科学アカデミー会員。\n",
      "8 他の主力製品はガラス繊維を用いた繊維強化プラスチック (FRP) などの複合材料である。\n",
      "9 また、このうち三浦、Nanami、有末、トリンドルは『めざましテレビ』(フジテレビ)の『MOTTOいまドキ!』にも出演していた。\n"
     ]
    }
   ],
   "source": [
    "# dataの確認\n",
    "print(unsup_train_dataset)\n",
    "\n",
    "print()\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "\n",
    "# シャッフルされているかの確認\n",
    "for i, text in enumerate(unsup_train_dataset[:10]['text']):\n",
    "    print(i, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6baca564-1107-4336-b147-a9edad998c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証データとテストデータの作成(JSTS)\n",
    "\n",
    "valid_dataset = load_dataset('llm-book/JGLUE', name='JSTS', split='train')\n",
    "test_dataset = load_dataset('llm-book/JGLUE', name='JSTS', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8f896ed-f2b8-434c-becf-f5062aa81bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/vocab.txt\n",
      "loading file spiece.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n"
     ]
    }
   ],
   "source": [
    "# tokenizer and collete function\n",
    "\n",
    "# Tokenizer\n",
    "base_model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e275098d-043a-4321-a4f5-1e279821ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate\n",
    "\n",
    "def unsup_train_collate_fn(\n",
    "    examples: list[dict],\n",
    ") -> dict[str, BatchEncoding | Tensor]:\n",
    "    \"\"\"教師なしSimCSEの訓練セットのミニバッチを作成\"\"\"\n",
    "    # ミニバッチに含まれる文にトークナイザを適用する\n",
    "    tokenized_texts = tokenizer(\n",
    "        [example[\"text\"] for example in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # 文と文の類似度行列における正例ペアの位置を示すTensorを作成する\n",
    "    # 行列のi行目の事例（文）に対してi列目の事例（文）との組が正例ペアとなる\n",
    "    labels = torch.arange(len(examples))\n",
    "\n",
    "    return {\n",
    "        \"tokenized_texts_1\": tokenized_texts,\n",
    "        \"tokenized_texts_2\": tokenized_texts,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad4b4b47-a228-4474-871a-9c58d9ae6d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_collate_fn(\n",
    "    examples: list[dict],\n",
    ") -> dict[str, BatchEncoding | Tensor]:\n",
    "    \"\"\"SimCSEの検証・テストセットのミニバッチを作成\"\"\"\n",
    "    # ミニバッチの文ペアに含まれる文（文1と文2）のそれぞれに\n",
    "    # トークナイザを適用する\n",
    "    tokenized_texts_1 = tokenizer(\n",
    "        [example[\"sentence1\"] for example in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized_texts_2 = tokenizer(\n",
    "        [example[\"sentence2\"] for example in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # 文1と文2の類似度行列における正例ペアの位置を示すTensorを作成する\n",
    "    # 行列のi行目の事例（文1）に対して\n",
    "    # i列目の事例（文2）との組が正例ペアとなる\n",
    "    labels = torch.arange(len(examples))\n",
    "\n",
    "    # データセットに付与された類似度スコアのTensorを作成する\n",
    "    label_scores = torch.tensor(\n",
    "        [example[\"label\"] for example in examples]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"tokenized_texts_1\": tokenized_texts_1,\n",
    "        \"tokenized_texts_2\": tokenized_texts_2,\n",
    "        \"labels\": labels,\n",
    "        \"label_scores\": label_scores,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c2add32-92c5-4df5-abfd-8c58c513327d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_pair_id': ['0', '1'],\n",
       " 'yjcaptions_id': ['10005_480798-10996-92616', '100124-104404-104405'],\n",
       " 'sentence1': ['川べりでサーフボードを持った人たちがいます。', '二人の男性がジャンボジェット機を見ています。'],\n",
       " 'sentence2': ['トイレの壁に黒いタオルがかけられています。', '2人の男性が、白い飛行機を眺めています。'],\n",
       " 'label': [0.0, 3.799999952316284]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f93278f8-2017-488d-9055-af91ae9093d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCSEModel(nn.Module):\n",
    "    def __init__(self, base_model_name: str, mlp_only_train: bool = False, temperature: float = 0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        self.hidden_size = self.encoder.config.hidden_size\n",
    "        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.actiovation = nn.Tanh()\n",
    "\n",
    "        self.mlp_only_train = mlp_only_train\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def encode_text(self, tokenized_texts: BatchEncoding) -> Tensor:\n",
    "        \"\"\"エンコーダーを使って文をベクトル変換\"\"\"\n",
    "\n",
    "        # トークン化されたテキストをエンコードする\n",
    "        encoded_texts = self.encoder(**tokenized_texts)\n",
    "        \n",
    "        # 最終層の[CLS]トークンのベクトルを取り出す ->  文ベクトルのスコア\n",
    "        encoded_texts = encoded_texts.last_hidden_state[:, 0]\n",
    "\n",
    "        # mlp_only_trainがFlseや推論の場合はヘッドにベクトルを渡さずに返す。\n",
    "        if self.mlp_only_train and not self.training:\n",
    "            return encoded_texts\n",
    "\n",
    "\n",
    "        # MLP層による変換\n",
    "        encoded_texts = self.dense(encoded_texts)\n",
    "        encoded_texts = self.actiovation(encoded_texts)\n",
    "\n",
    "        return encoded_texts\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        tokenized_texts_1: BatchEncoding,\n",
    "        tokenized_texts_2: BatchEncoding,\n",
    "        labels : Tensor,\n",
    "        label_scores: Tensor | None = None,\n",
    "    ) -> ModelOutput:\n",
    "        \n",
    "        # 各トークン化されたテキストを文ベクトルに変換\n",
    "        encoded_texts_1 = self.encode_text(tokenized_texts_1)\n",
    "        encoded_texts_2 = self.encode_text(tokenized_texts_2)\n",
    "\n",
    "        # コサイン類似度を算出\n",
    "        #  tokenized_texts_2.unsqueeze(0)はブロードキャストを利用？？\n",
    "        # output-> (batch_size, batch_sizeの大きさのscores): 類似度行列\n",
    "        sim_matrix = F.cosine_similarity(encoded_texts_1.unsqueeze(1),encoded_texts_2.unsqueeze(0),dim=2,)\n",
    "        \n",
    "        loss = F.cross_entropy(sim_matrix / self.temperature, labels)\n",
    "\n",
    "        # positive score\n",
    "        positive_mask = F.one_hot(labels, sim_matrix.size(1)).bool()\n",
    "        positive_scores = torch.masked_select(sim_matrix, positive_mask)\n",
    "\n",
    "        \n",
    "        return ModelOutput(loss=loss, scores=positive_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "86596753-02fa-48b7-a9b1-819e0c27b85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-v3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/pytorch_model.bin\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v3 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "unsup_model = SimCSEModel(base_model_name, mlp_only_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0fc59fa9-9257-4871-8ad7-b76cf9079a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# モデルの訓練と評価をするTrainerを準備\n",
    "# 今回評価指標として、スペアマンの順位相関係数を使用\n",
    "\n",
    "# 評価結果の計算関数\n",
    "def compute_metrics(p: EvalPrediction) -> dict[str, float]:\n",
    "    \"\"\"モデルの予想スコアと評価用スコアのスピアマン順位相関係数を計算\"\"\"\n",
    "\n",
    "    scores = p.predictions\n",
    "    labels, label_scores = p.label_ids\n",
    "\n",
    "    spearman = spearmanr(scores, label_scores).statistic\n",
    "\n",
    "    return {'spearman': spearman}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "df6ecbeb-df47-4509-b228-faf85b85af48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "unsup_training_args = TrainingArguments(\n",
    "    output_dir=\"outputs_unsup_simcse\",  # 結果の保存先フォルダ\n",
    "    per_device_train_batch_size=64,  # 訓練時のバッチサイズ\n",
    "    per_device_eval_batch_size=64,  # 評価時のバッチサイズ\n",
    "    learning_rate=3e-5,  # 学習率\n",
    "    num_train_epochs=1,  # 訓練エポック数\n",
    "    evaluation_strategy=\"steps\",  # 検証セットによる評価のタイミング\n",
    "    eval_steps=250,  # 検証セットによる評価を行う訓練ステップ数の間隔\n",
    "    logging_steps=250,  # ロギングを行う訓練ステップ数の間隔\n",
    "    save_steps=250,  # チェックポイントを保存する訓練ステップ数の間隔\n",
    "    save_total_limit=1,  # 保存するチェックポイントの最大数\n",
    "    fp16=True,  # 自動混合精度演算の有効化\n",
    "    load_best_model_at_end=True,  # 最良のモデルを訓練終了後に読み込むか\n",
    "    metric_for_best_model=\"spearman\",  # 最良のモデルを決定する評価指標\n",
    "    remove_unused_columns=False,  # データセットの不要フィールドを削除するか\n",
    "    report_to='all'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "53ad284a-b32d-4c42-8b49-e7620d2a0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練セットと検証セットで異なるcollate関数を使用するのでカスタムtrainerを作成\n",
    "\n",
    "class SimCSETrainer(Trainer):\n",
    "    \"\"\"SimCSEの訓練に使用するTrainer\"\"\"\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset: Dataset | None = None) -> DataLoader:\n",
    "\n",
    "        if eval_dataset is None:\n",
    "            eval_dataset = self.eval_dataset\n",
    "\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=64,\n",
    "            collate_fn=eval_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "080d1497-3039-42c1-88e9-b21c77897e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "# 教師なしSimCSEのTrainerを初期化する\n",
    "unsup_trainer = SimCSETrainer(model=unsup_model,\n",
    "                              args=unsup_training_args,\n",
    "                              data_collator=unsup_train_collate_fn,\n",
    "                              train_dataset=unsup_train_dataset,\n",
    "                              eval_dataset=valid_dataset,\n",
    "                              compute_metrics=compute_metrics\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b749e18e-6971-4fb2-b889-5a9858e38e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1,000,000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15,625\n",
      "  Number of trainable parameters = 111,797,760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15625' max='15625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15625/15625 49:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>2.516721</td>\n",
       "      <td>0.716522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.461056</td>\n",
       "      <td>0.735719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.478571</td>\n",
       "      <td>0.733308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.458963</td>\n",
       "      <td>0.739426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.500067</td>\n",
       "      <td>0.729833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.474350</td>\n",
       "      <td>0.726624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.490425</td>\n",
       "      <td>0.726557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.448895</td>\n",
       "      <td>0.733077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.381986</td>\n",
       "      <td>0.734255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.427408</td>\n",
       "      <td>0.728923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.420591</td>\n",
       "      <td>0.729394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.362568</td>\n",
       "      <td>0.744062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.337173</td>\n",
       "      <td>0.747023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.324284</td>\n",
       "      <td>0.749841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.319464</td>\n",
       "      <td>0.750396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.287964</td>\n",
       "      <td>0.752308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.302680</td>\n",
       "      <td>0.756469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.292817</td>\n",
       "      <td>0.756861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.288189</td>\n",
       "      <td>0.757201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.265404</td>\n",
       "      <td>0.757524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.305733</td>\n",
       "      <td>0.756851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.308980</td>\n",
       "      <td>0.754869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.332468</td>\n",
       "      <td>0.746834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.276469</td>\n",
       "      <td>0.748431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.273878</td>\n",
       "      <td>0.751198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.280088</td>\n",
       "      <td>0.747615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.318081</td>\n",
       "      <td>0.742352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.292228</td>\n",
       "      <td>0.749239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.281392</td>\n",
       "      <td>0.751655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.274431</td>\n",
       "      <td>0.753844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.276212</td>\n",
       "      <td>0.756702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.275770</td>\n",
       "      <td>0.756032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.268899</td>\n",
       "      <td>0.757646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.267805</td>\n",
       "      <td>0.756641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.243706</td>\n",
       "      <td>0.754486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.255006</td>\n",
       "      <td>0.751331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.226253</td>\n",
       "      <td>0.754097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.224980</td>\n",
       "      <td>0.754247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.220426</td>\n",
       "      <td>0.752008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.238368</td>\n",
       "      <td>0.749029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.241981</td>\n",
       "      <td>0.749913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.249483</td>\n",
       "      <td>0.748962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>2.329623</td>\n",
       "      <td>0.743336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.339920</td>\n",
       "      <td>0.744674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.321412</td>\n",
       "      <td>0.748549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.318005</td>\n",
       "      <td>0.748251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.320564</td>\n",
       "      <td>0.745387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.323854</td>\n",
       "      <td>0.744880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.327187</td>\n",
       "      <td>0.744872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.328646</td>\n",
       "      <td>0.744625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.327474</td>\n",
       "      <td>0.744521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.324352</td>\n",
       "      <td>0.744872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.321848</td>\n",
       "      <td>0.744120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.339337</td>\n",
       "      <td>0.740794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.331940</td>\n",
       "      <td>0.741513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.331630</td>\n",
       "      <td>0.740898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.324224</td>\n",
       "      <td>0.741222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.316265</td>\n",
       "      <td>0.740413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.313283</td>\n",
       "      <td>0.739562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.312941</td>\n",
       "      <td>0.739829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.313074</td>\n",
       "      <td>0.739860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.313709</td>\n",
       "      <td>0.739865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-1000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-500] due to args.save_total_limit\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-1250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-1500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-1250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-1750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-2000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-1750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-2250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-2500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-2250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-2750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-2500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-3000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-1000] due to args.save_total_limit\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-2750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-3250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-3500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-3250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-3750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-3500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-4000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-3750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-4250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-4500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-4250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-4750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-4500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-5000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-4750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-5250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-5500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-5250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-5750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-5500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-6000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-5750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-6250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-6500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-6250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-6750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-6500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-7000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-6750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-7250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-7000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-7500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-7250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-7750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-7500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-8000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-7750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-8250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-5000] due to args.save_total_limit\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-8500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-8750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-8500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-9000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-8750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-9250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-9000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-9500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-9250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-9750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-9500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-10000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-9750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-10250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-10500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-10250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-10750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-10500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-11000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-10750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-11250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-11000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-11500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-11250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-11750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-11500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-12000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-11750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-12250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-12500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-12250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-12750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-12500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-13000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-12750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-13250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-13000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-13500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-13250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-13750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-13500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-14000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-13750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-14250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-14500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-14250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-14750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-14500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-15000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-14750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-15250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to outputs_unsup_simcse/checkpoint-15500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-15250] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs_unsup_simcse/checkpoint-8250 (score: 0.7576461344472822).\n",
      "Deleting older checkpoint [outputs_unsup_simcse/checkpoint-15500] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15625, training_loss=0.00012896459624171257, metrics={'train_runtime': 2971.4218, 'train_samples_per_second': 336.539, 'train_steps_per_second': 5.258, 'total_flos': 0.0, 'train_loss': 0.00012896459624171257, 'epoch': 1.0})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練の実行\n",
    "unsup_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b8459a8c-9b19-41fa-8d79-8e6af6cca066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12451\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [195/195 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.2688992023468018,\n",
       " 'eval_spearman': 0.7576461344472822,\n",
       " 'eval_runtime': 9.7325,\n",
       " 'eval_samples_per_second': 1279.319,\n",
       " 'eval_steps_per_second': 20.036,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの評価\n",
    "unsup_trainer.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bcb4cad7-2de4-4e95-b005-78673afe0d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1457\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.139267683029175,\n",
       " 'eval_spearman': 0.7969628926980149,\n",
       " 'eval_runtime': 1.3159,\n",
       " 'eval_samples_per_second': 1107.189,\n",
       " 'eval_steps_per_second': 17.478,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsup_trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8c05f11c-0236-4e0e-81fc-fee267235a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in outputs_unsup_simcse/encoder/config.json\n",
      "Model weights saved in outputs_unsup_simcse/encoder/model.safetensors\n",
      "tokenizer config file saved in outputs_unsup_simcse/encoder/tokenizer_config.json\n",
      "Special tokens file saved in outputs_unsup_simcse/encoder/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('outputs_unsup_simcse/encoder/tokenizer_config.json',\n",
       " 'outputs_unsup_simcse/encoder/special_tokens_map.json',\n",
       " 'outputs_unsup_simcse/encoder/vocab.txt',\n",
       " 'outputs_unsup_simcse/encoder/added_tokens.json')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_path = \"outputs_unsup_simcse/encoder\"\n",
    "unsup_model.encoder.save_pretrained(encoder_path)\n",
    "tokenizer.save_pretrained(encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf286ee-1e84-46c6-91dd-c152bbca189b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12531095-d8c8-4f4b-9787-7c01f2f26f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25bebe-f0fd-4f01-a267-84afda6c1857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb8d9c-eef4-4b11-a075-57ac682f05b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929c045-c9a1-4dca-a926-7c55471e1563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66f411-d6fd-4e2e-8eb8-c9f81c71c500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe9ea8-1a4b-4e53-9e88-4239d674d33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f7ed0-9831-47ea-b0b3-296d28383726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffbf7f9-e16b-4979-a9d2-acdeacfa7eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81573b92-f752-43e5-8dad-e6f7e579f372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9912fe-6c85-43f5-b088-fef895e80496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f7594-a6bf-4178-a8fd-b7bec52101f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc72b2-f632-4b9d-bacd-d490e214bf12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd4cd7-ad75-44b4-b87a-55fc462abcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
