{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "46ab473e-c1cd-4a6a-a192-1eaba4eeffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from unicodedata import normalize, is_normalized\n",
    "from spacy_alignments.tokenizations import get_alignments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer,BatchEncoding, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b0391db-eaf0-4ab6-8200-17e6875ad432",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('llm-book/ner-wikipedia-dataset', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1cab38-7596-4229-bc6c-6990220cf89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 4274\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 534\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 535\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2699452e-f562-4fe0-9af7-01ccf20889a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'curid': '3638038',\n",
      "  'entities': [{'name': 'さくら学院', 'span': [0, 5], 'type': 'その他の組織名'},\n",
      "               {'name': 'Ciao Smiles', 'span': [6, 17], 'type': 'その他の組織名'}],\n",
      "  'text': 'さくら学院、Ciao Smilesのメンバー。'},\n",
      " {'curid': '1729527',\n",
      "  'entities': [{'name': 'レクレアティーボ・ウェルバ', 'span': [17, 30], 'type': 'その他の組織名'},\n",
      "               {'name': 'プリメーラ・ディビシオン', 'span': [32, 44], 'type': 'その他の組織名'}],\n",
      "  'text': '2008年10月5日、アウェーでのレクレアティーボ・ウェルバ戦でプリメーラ・ディビシオンでの初得点を決めた。'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(dataset['train'])[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98fdbd19-1010-4769-8fe1-b681da08f3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'curid': '3638038',\n",
      " 'entities': [{'name': 'さくら学院', 'span': [0, 5], 'type': 'その他の組織名'},\n",
      "              {'name': 'Ciao Smiles', 'span': [6, 17], 'type': 'その他の組織名'}],\n",
      " 'text': 'さくら学院、Ciao Smilesのメンバー。'}\n"
     ]
    }
   ],
   "source": [
    "for i in dataset['train']:\n",
    "    pprint(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae13559-00ed-4f0a-a19e-e2f03d283f97",
   "metadata": {},
   "source": [
    "・今回はテキストに含まれる固有表現のスパンとそのタイプを指定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5eeb6e1-b3fa-4896-b908-9c6a9d9d7ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>人名</th>\n",
       "      <td>2394</td>\n",
       "      <td>299</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>法人名</th>\n",
       "      <td>2006</td>\n",
       "      <td>231</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>地名</th>\n",
       "      <td>1769</td>\n",
       "      <td>184</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>政治的組織名</th>\n",
       "      <td>953</td>\n",
       "      <td>121</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>製品名</th>\n",
       "      <td>934</td>\n",
       "      <td>123</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>施設名</th>\n",
       "      <td>868</td>\n",
       "      <td>103</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>その他の組織名</th>\n",
       "      <td>852</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>イベント名</th>\n",
       "      <td>831</td>\n",
       "      <td>85</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>合計</th>\n",
       "      <td>10607</td>\n",
       "      <td>1245</td>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train  validation  test\n",
       "人名        2394         299   287\n",
       "法人名       2006         231   248\n",
       "地名        1769         184   204\n",
       "政治的組織名     953         121   106\n",
       "製品名        934         123   158\n",
       "施設名        868         103   137\n",
       "その他の組織名    852          99   100\n",
       "イベント名      831          85    93\n",
       "合計       10607        1245  1333"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データセットの分析\n",
    "\n",
    "def count_label_occurrences(dataset: Dataset) -> dict[str, int]:\n",
    "\n",
    "    # 固有表現タイプを抽出したlistを作成する\n",
    "    entities = [\n",
    "        e['type'] for data in dataset for e in data['entities']\n",
    "    ]\n",
    "\n",
    "    # ラベルの出現回数が多い順に並び変える\n",
    "    # Counterにはmost_common()メソッドがあり、(要素, 出現回数)という形のタプルを出現回数順に並べたリストを返す。\n",
    "    label_counts = dict(Counter(entities).most_common())\n",
    "    return label_counts\n",
    "\n",
    "\n",
    "label_counts_dict = {}\n",
    "for split in dataset:\n",
    "    label_counts_dict[split] = count_label_occurrences(dataset[split])\n",
    "df = pd.DataFrame(label_counts_dict)\n",
    "df.loc['合計'] = df.sum()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b099caf-d4fa-4f29-b366-0a992ef232d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainにおけるスパンが重複する事例数：0\n",
      "validationにおけるスパンが重複する事例数：0\n",
      "testにおけるスパンが重複する事例数：0\n"
     ]
    }
   ],
   "source": [
    "def has_overlap(spans):\n",
    "    sorted_spans = sorted(spans, key=lambda x: x[0])\n",
    "    for i in range(1, len(sorted_spans)):\n",
    "        if sorted_spans[i-1][1] > sorted_spans[i][0]:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "overlap_count = 0\n",
    "for split in dataset:\n",
    "    for data in dataset[split]:\n",
    "        if data['entities']:\n",
    "            spans = [e['span'] for e in data['entities']]\n",
    "            overlap_count += has_overlap(spans)\n",
    "\n",
    "    print(f\"{split}におけるスパンが重複する事例数：{overlap_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e48ecde-4ee1-4b33-b094-bfc4dc0d6432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 9], [10, 21], [25, 37]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed4e5f2f-afbd-44c0-a454-9fe9539b04f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'ダーヴラ・カーワン', 'span': [0, 9], 'type': '人名'},\n",
       " {'name': 'マーシー・ハーティガン', 'span': [10, 21], 'type': '人名'},\n",
       " {'name': 'ラッセル・T・デイヴィス', 'span': [25, 37], 'type': '人名'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "979ef1c3-7ae8-46e9-9e88-66bb3ef0b82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'curid': ['4113413'],\n",
       " 'text': ['ダーヴラ・カーワンはマーシー・ハーティガンを演じ、ラッセル・T・デイヴィスは本作のポッドキャストコメンタリーで彼女について「これまでにないほどダークな悪役」と表現した。'],\n",
       " 'entities': [[{'name': 'ダーヴラ・カーワン', 'span': [0, 9], 'type': '人名'},\n",
       "   {'name': 'マーシー・ハーティガン', 'span': [10, 21], 'type': '人名'},\n",
       "   {'name': 'ラッセル・T・デイヴィス', 'span': [25, 37], 'type': '人名'}]]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68690508-bef7-40eb-9039-2583d8473208",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc749fb-bd07-4a2c-bdcb-389afd1331d4",
   "metadata": {},
   "source": [
    "## テキスト正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2286c0e3-0f1e-4598-88e3-e1995f7a3369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正規化前 ABCＡＢＣabcABCアイウｱｲｳ①②③123\n",
      "正規化後 ABCABCabcABCアイウアイウ123123\n"
     ]
    }
   ],
   "source": [
    "text = \"ABCＡＢＣabcABCアイウｱｲｳ①②③123\"\n",
    "\n",
    "nomalized_text = normalize('NFKC', text)\n",
    "print('正規化前', text)\n",
    "print('正規化後', nomalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9e1edc5-c517-4cb9-bac9-a5553181fce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正規化されていない事例数: 0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for split in dataset:\n",
    "    for data in dataset[split]:\n",
    "        if not is_normalized('NFKC',data['text']): # 正規化されていないとFalseをかえす？\n",
    "            count += 1\n",
    "print(f'正規化されていない事例数: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee09b72c-2c0a-4540-a034-49a39a36acf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"ABCＡＢＣabcABCアイウｱｲｳ①②③123\"\n",
    "is_normalized('NFKC', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a64dc447-b952-4090-a20a-409632b7433a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'さ/く/ら/学/院/、/C/i/a/o/ /S/m/i/l/e/s/の/メ/ン/バ/ー/。'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'.join(dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86a44d-65a4-42d9-93e9-7e4880b13081",
   "metadata": {},
   "source": [
    "## 文字列とトークン列のアライメント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "87b0de43-90e3-4b9c-a5e4-f40e7dc3364e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['さ', 'く', 'ら', '学', '院'] ['[CLS]', 'さくら', '学院', '[SEP]']\n",
      "文字に対するトークンの位置 [[1], [1], [1], [2], [2]]\n",
      "トークンに対する文字の位置 [[], [0, 1, 2], [3, 4], []]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tohoku-nlp/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "text ='さくら学院'\n",
    "\n",
    "# 文字列のLISTに変換\n",
    "characters = list(text)\n",
    "\n",
    "# 特殊トークンも含めたリストにする\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "\n",
    "char_to_token_indices, token_to_char_indices = get_alignments(characters, tokens)\n",
    "print(characters, tokens)\n",
    "print('文字に対するトークンの位置',char_to_token_indices)\n",
    "print('トークンに対する文字の位置',token_to_char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "200d2424-c864-430b-96ec-4da8ae2ffec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['さ', 'く', 'ら', '学', '院'] ['[CLS]', 'さくら', '学院', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(characters, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6144f2d7-ffc9-4ef8-b86e-042a6d766ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '大谷翔平は岩手県水沢市出身'\n",
    "entities = [\n",
    "    {'name':'大谷正平', 'span':[0, 4], 'type':'人名'},\n",
    "    {'name':'岩手県水沢市', 'span':[5, 11], 'type':'地方'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "17ac174a-8a1f-4a38-bad1-f5115aea8640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>位置</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>トークン列</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>大谷</td>\n",
       "      <td>翔</td>\n",
       "      <td>##平</td>\n",
       "      <td>は</td>\n",
       "      <td>岩手</td>\n",
       "      <td>県</td>\n",
       "      <td>水沢</td>\n",
       "      <td>市</td>\n",
       "      <td>出身</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ラベル列</th>\n",
       "      <td>-</td>\n",
       "      <td>B-人名</td>\n",
       "      <td>I-人名</td>\n",
       "      <td>I-人名</td>\n",
       "      <td>0</td>\n",
       "      <td>B-地方</td>\n",
       "      <td>I-地方</td>\n",
       "      <td>I-地方</td>\n",
       "      <td>I-地方</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "位置        0     1     2     3  4     5     6     7     8   9      10\n",
       "トークン列  [CLS]    大谷     翔   ##平  は    岩手     県    水沢     市  出身  [SEP]\n",
       "ラベル列       -  B-人名  I-人名  I-人名  0  B-地方  I-地方  I-地方  I-地方   0      -"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def output_tokens_and_labels(text, entities, tokenizer):\n",
    "    characters = list(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "    char_to_token_indices, _ = get_alignments(characters, tokens)\n",
    "    \n",
    "    # 0で初期化したラベルリスト\n",
    "    labels = ['0'] * len(tokens)\n",
    "    for entity in entities:\n",
    "        entity_span, entity_type = entity['span'], entity['type']\n",
    "        start = char_to_token_indices[entity_span[0]][0]\n",
    "        end = char_to_token_indices[entity_span[1] -1][0]\n",
    "        labels[start] = f\"B-{entity_type}\"\n",
    "        for idx in range(start + 1, end + 1):\n",
    "            labels[idx] = f\"I-{entity_type}\"\n",
    "    \n",
    "        labels[0] = '-'\n",
    "        labels[-1] = '-'\n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "tokens, labels = output_tokens_and_labels(text, entities, tokenizer)\n",
    "\n",
    "df = pd.DataFrame({'トークン列':tokens, 'ラベル列':labels})\n",
    "df.index.name = \"位置\"\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0616e128-055a-432e-b302-11a2200ab120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "def create_character_labels(\n",
    "    text: str, entities: list[dict[str, list[int] | str]]\n",
    ") -> list[str]:\n",
    "    \"\"\"文字ベースでラベルのlistを作成\"\"\"\n",
    "    # \"O\"のラベルで初期化したラベルのlistを作成する\n",
    "    labels = [\"O\"] * len(text)\n",
    "    for entity in entities: # 各固有表現を処理する\n",
    "        entity_span, entity_type = entity[\"span\"], entity[\"type\"]\n",
    "        # 固有表現の開始文字の位置に\"B-\"のラベルを設定する\n",
    "        labels[entity_span[0]] = f\"B-{entity_type}\"\n",
    "        # 固有表現の開始文字以外の位置に\"I-\"のラベルを設定する\n",
    "        for i in range(entity_span[0] + 1, entity_span[1]):\n",
    "            labels[i] = f\"I-{entity_type}\"\n",
    "    return labels\n",
    "\n",
    "def convert_results_to_labels(\n",
    "    results: list[dict[str, Any]]\n",
    ") -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"正解データと予測データのラベルのlistを作成\"\"\"\n",
    "    true_labels, pred_labels = [], []\n",
    "    for result in results: # 各事例を処理する\n",
    "        # 文字ベースでラベルのリストを作成してlistに加える\n",
    "        true_labels.append(\n",
    "            create_character_labels(result[\"text\"], result[\"entities\"])\n",
    "        )\n",
    "        pred_labels.append(\n",
    "            create_character_labels(result[\"text\"], result[\"pred_entities\"])\n",
    "        )\n",
    "    return true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b5a47a3e-7d77-4e7d-8c7a-e748b5097d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          人名       1.00      1.00      1.00         1\n",
      "          地名       0.00      0.00      0.00         1\n",
      "         施設名       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.33      0.50      0.40         2\n",
      "   macro avg       0.33      0.33      0.33         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 評価指標のseqevalの挙動\n",
    "\n",
    "results = [\n",
    "    {\n",
    "        \"text\": \"大谷翔平は岩手県水沢市出身\",\n",
    "        \"entities\": [\n",
    "            {\"name\": \"大谷翔平\", \"span\": [0, 4], \"type\": \"人名\"},\n",
    "            {\"name\": \"岩手県水沢市\", \"span\": [5, 11], \"type\": \"地名\"},\n",
    "        ],\n",
    "        \"pred_entities\": [\n",
    "            {\"name\": \"大谷翔平\", \"span\": [0, 4], \"type\": \"人名\"},\n",
    "            {\"name\": \"岩手県\", \"span\": [5, 8], \"type\": \"地名\"},\n",
    "            {\"name\": \"水沢市\", \"span\": [8, 11], \"type\": \"施設名\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "true_labels, pred_labels = convert_results_to_labels(results)\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d1f4955c-6999-4793-b244-162b71acba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.3333333333333333, 'recall': 0.5, 'F1-score': 0.4}\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_scores(true_labels: list[list[str]], pred_labels: list[list[str]], average:str) -> dict[str, float]:\n",
    "    scores = {\n",
    "        'precision': precision_score(true_labels, pred_labels, average=average),\n",
    "        'recall': recall_score(true_labels, pred_labels, average=average),\n",
    "        'F1-score': f1_score(true_labels, pred_labels, average=average),\n",
    "    }\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "print(compute_scores(true_labels, pred_labels, 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb959be3-3227-4737-a573-14a9cf873ca6",
   "metadata": {},
   "source": [
    "## 固有表現認識モデルの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "12a19ec0-49d6-490d-964e-52bc6b115fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTのファインチューニング\n",
    "\n",
    "# label1id\n",
    "def create_label2id(\n",
    "    entities_list: list[list[dict[str, str | str]]]\n",
    ") -> dict[str, int]:\n",
    "    label2id = {\"0\": 0}\n",
    "\n",
    "    # setなので重複はなし\n",
    "    entity_type = set([e['type'] for entities in entities_list for e in entities])\n",
    "\n",
    "    entity_types = sorted(entity_type)\n",
    "\n",
    "    # 1entityにつき2種類登録\n",
    "    for i, entity_type in enumerate(entity_types):\n",
    "        label2id[f\"B-{entity_type}\"] = i*2 + 1\n",
    "        label2id[f\"I-{entity_type}\"] = i*2 + 2\n",
    "    return label2id\n",
    "\n",
    "\n",
    "label2id = create_label2id(dataset['train'][\"entities\"])\n",
    "id2label = {id:v for v, id in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "832b36af-d7cf-4df5-b399-3165b6fae721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# データの前処理\n",
    "\n",
    "def preprocess_data(data, tokenizer, label2id) -> BatchEncoding:\n",
    "    # トークナイゼーション\n",
    "    inputs = tokenizer(data['text'], return_tensors='pt', return_special_tokens_mask=True)\n",
    "    inputs = { k:v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "    characters = list(data['text'])\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'])\n",
    "    print(characters)\n",
    "    print(tokens)\n",
    "    char_to_token_indeces, _ = get_alignments(characters, tokens)\n",
    "\n",
    "    labels = torch.zeros_like(inputs['input_ids'])\n",
    "    for entity in data['entities']:\n",
    "        print(char_to_token_indeces)\n",
    "        print(entity['span'][0])\n",
    "        print(entity['span'][1] - 1)\n",
    "        start_token_indeces = char_to_token_indeces[entity['span'][0]]\n",
    "        end_token_indeces = char_to_token_indeces[entity['span'][1] - 1]\n",
    "\n",
    "        # 文字に対応するトークンが存在しなければスキップ -> 固有表現ではない単語 ex(は)\n",
    "        if(\n",
    "            len(start_token_indeces) == 0\n",
    "            or len(end_token_indeces) == 0\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        start, end = start_token_indeces[0], end_token_indeces[0]\n",
    "        print(start, end)\n",
    "        entity_type = entity['type']\n",
    "\n",
    "        labels[start] = label2id[f\"B-{entity_type}\"]\n",
    "        if start != end:\n",
    "            labels[start + 1 : end + 1] = label2id[f\"I-{entity_type}\"]\n",
    "\n",
    "\n",
    "        labels[torch.where(inputs[\"special_tokens_mask\"])] = -100\n",
    "        inputs['labels'] = labels\n",
    "    return inputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "26477da6-d83d-44a9-ae32-aab226aa6c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['さ', 'く', 'ら', '学', '院', '、', 'C', 'i', 'a', 'o', ' ', 'S', 'm', 'i', 'l', 'e', 's', 'の', 'メ', 'ン', 'バ', 'ー', '。']\n",
      "['[CLS]', 'さくら', '学院', '、', 'C', '##ia', '##o', 'Sm', '##ile', '##s', 'の', 'メンバー', '。', '[SEP]']\n",
      "[[1], [1], [1], [2], [2], [3], [4], [5], [5], [6], [], [7], [7], [8], [8], [8], [9], [10], [11], [11], [11], [11], [12]]\n",
      "0\n",
      "4\n",
      "1 2\n",
      "[[1], [1], [1], [2], [2], [3], [4], [5], [5], [6], [], [7], [7], [8], [8], [8], [9], [10], [11], [11], [11], [11], [12]]\n",
      "6\n",
      "16\n",
      "4 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2, 16972, 14284,   384,    50, 13634,  7075, 20218, 18124,  7045,\n",
       "           464, 12913,   385,     3]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'special_tokens_mask': tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([-100,    1,    2,    0,    1,    2,    2,    2,    2,    2,    0,    0,\n",
       "            0, -100])}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = dataset['train'][0]\n",
    "preprocess_data(test, tokenizer, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "54d2296b-e3aa-44f3-a859-dd58eb0d8717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'curid': '3638038',\n",
      " 'entities': [{'name': 'さくら学院', 'span': [0, 5], 'type': 'その他の組織名'},\n",
      "              {'name': 'Ciao Smiles', 'span': [6, 17], 'type': 'その他の組織名'}],\n",
      " 'text': 'さくら学院、Ciao Smilesのメンバー。'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c81af893-2f30-40be-8267-58a0b1288f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05264e545ffc43148deed515ed8b8d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4296a0e8f0f4516a52584eb03afde0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 訓練セットに前処理\n",
    "train_dataset = dataset['train'].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\":tokenizer,\n",
    "        \"label2id\":label2id\n",
    "    }, remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "eval_dataset = dataset['validation'].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\":tokenizer,\n",
    "        \"label2id\":label2id\n",
    "    }, remove_columns=dataset['validation'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "68c743fb-d46b-4897-9c92-6926e5e42894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tohoku-nlp--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"tohoku-nlp/bert-base-japanese-v3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"2\": \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"3\": \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"4\": \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"5\": \"B-\\u4eba\\u540d\",\n",
      "    \"6\": \"I-\\u4eba\\u540d\",\n",
      "    \"7\": \"B-\\u5730\\u540d\",\n",
      "    \"8\": \"I-\\u5730\\u540d\",\n",
      "    \"9\": \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"10\": \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"11\": \"B-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"12\": \"I-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"13\": \"B-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"14\": \"I-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"15\": \"B-\\u88fd\\u54c1\\u540d\",\n",
      "    \"16\": \"I-\\u88fd\\u54c1\\u540d\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 1,\n",
      "    \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 3,\n",
      "    \"B-\\u4eba\\u540d\": 5,\n",
      "    \"B-\\u5730\\u540d\": 7,\n",
      "    \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 9,\n",
      "    \"B-\\u65bd\\u8a2d\\u540d\": 11,\n",
      "    \"B-\\u6cd5\\u4eba\\u540d\": 13,\n",
      "    \"B-\\u88fd\\u54c1\\u540d\": 15,\n",
      "    \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 2,\n",
      "    \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 4,\n",
      "    \"I-\\u4eba\\u540d\": 6,\n",
      "    \"I-\\u5730\\u540d\": 8,\n",
      "    \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 10,\n",
      "    \"I-\\u65bd\\u8a2d\\u540d\": 12,\n",
      "    \"I-\\u6cd5\\u4eba\\u540d\": 14,\n",
      "    \"I-\\u88fd\\u54c1\\u540d\": 16\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--tohoku-nlp--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/pytorch_model.bin\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-v3 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, label2id=label2id, id2label=id2label)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d1616dc6-7e01-4cf1-9047-3918a6a1481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using auto half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 4,274\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 670\n",
      "  Number of trainable parameters = 110,629,649\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 01:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.686200</td>\n",
       "      <td>0.100481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.091146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.087163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.092064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.097529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to output_bert_ner/checkpoint-134\n",
      "Configuration saved in output_bert_ner/checkpoint-134/config.json\n",
      "Model weights saved in output_bert_ner/checkpoint-134/model.safetensors\n",
      "tokenizer config file saved in output_bert_ner/checkpoint-134/tokenizer_config.json\n",
      "Special tokens file saved in output_bert_ner/checkpoint-134/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to output_bert_ner/checkpoint-268\n",
      "Configuration saved in output_bert_ner/checkpoint-268/config.json\n",
      "Model weights saved in output_bert_ner/checkpoint-268/model.safetensors\n",
      "tokenizer config file saved in output_bert_ner/checkpoint-268/tokenizer_config.json\n",
      "Special tokens file saved in output_bert_ner/checkpoint-268/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to output_bert_ner/checkpoint-402\n",
      "Configuration saved in output_bert_ner/checkpoint-402/config.json\n",
      "Model weights saved in output_bert_ner/checkpoint-402/model.safetensors\n",
      "tokenizer config file saved in output_bert_ner/checkpoint-402/tokenizer_config.json\n",
      "Special tokens file saved in output_bert_ner/checkpoint-402/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to output_bert_ner/checkpoint-536\n",
      "Configuration saved in output_bert_ner/checkpoint-536/config.json\n",
      "Model weights saved in output_bert_ner/checkpoint-536/model.safetensors\n",
      "tokenizer config file saved in output_bert_ner/checkpoint-536/tokenizer_config.json\n",
      "Special tokens file saved in output_bert_ner/checkpoint-536/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to output_bert_ner/checkpoint-670\n",
      "Configuration saved in output_bert_ner/checkpoint-670/config.json\n",
      "Model weights saved in output_bert_ner/checkpoint-670/model.safetensors\n",
      "tokenizer config file saved in output_bert_ner/checkpoint-670/tokenizer_config.json\n",
      "Special tokens file saved in output_bert_ner/checkpoint-670/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=670, training_loss=0.16173771913371868, metrics={'train_runtime': 86.146, 'train_samples_per_second': 248.067, 'train_steps_per_second': 7.777, 'total_flos': 1070012411245680.0, 'train_loss': 0.16173771913371868, 'epoch': 5.0})"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='output_bert_ner',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type='linear',\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "1d35fd77-52bd-48b4-add0-14a98e78b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力: {'input_ids': [[0, 1], [2, 3]], 'labels': [[1, 2], [3, 4]]}\n",
      "出力: [{'input_ids': [0, 1], 'labels': [1, 2]}, {'input_ids': [2, 3], 'labels': [3, 4]}]\n"
     ]
    }
   ],
   "source": [
    "def convert_list_dict_to_dict_list(\n",
    "    list_dict: dict[str, list]\n",
    ") -> list[dict[str, list]]:\n",
    "    \"\"\"ミニバッチのデータを事例単位のlistに変換\"\"\"\n",
    "    dict_list = []\n",
    "    # dictのキーのlistを作成する\n",
    "    keys = list(list_dict.keys())\n",
    "    for idx in range(len(list_dict[keys[0]])): # 各事例で処理する\n",
    "        # dictの各キーからデータを取り出してlistに追加する\n",
    "        dict_list.append({key: list_dict[key][idx] for key in keys})\n",
    "    return dict_list\n",
    "\n",
    "# ミニバッチのデータを事例単位のlistに変換する\n",
    "list_dict = {\n",
    "    \"input_ids\": [[0, 1], [2, 3]],\n",
    "    \"labels\": [[1, 2], [3, 4]],\n",
    "}\n",
    "dict_list = convert_list_dict_to_dict_list(list_dict)\n",
    "print(f\"入力: {list_dict}\")\n",
    "print(f\"出力: {dict_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "c758c1f2-8460-4eed-86e7-e620566072d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'labels']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " list(list_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "e94593ee-9d93-4d76-b179-d5b9f878b59b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:00<00:00, 29.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 15, 16, 0, 0, 13, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 14, 14, 13, 0, 0, 0, 0, 0, 15, 15, 16, 15, 13, 14, 14, 14, 14, 13, 13, 13, 14, 14, 13, 0, 0, 13, 13, 14, 14, 14, 13, 0, 13, 14, 14, 14, 0, 0, 0, 13, 0, 15, 16, 16, 0, 13, 14, 14, 14, 14, 13, 0, 0, 0, 15, 15, 16, 16, 13, 13, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "def run_prediction(\n",
    "    dataloader: DataLoader, model: PreTrainedModel\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"予測スコアに基づき固有表現ラベルを予測\"\"\"\n",
    "    predictions = []\n",
    "    for batch in tqdm(dataloader): # 各ミニバッチを処理する\n",
    "        inputs = {\n",
    "            k: v.to(model.device)\n",
    "            for k, v in batch.items()\n",
    "            if k != \"special_tokens_mask\"\n",
    "        }\n",
    "        # 予測スコアを取得する\n",
    "        logits = model(**inputs).logits\n",
    "        # 最もスコアの高いIDを取得する\n",
    "        batch[\"pred_label_ids\"] = logits.argmax(-1)\n",
    "        batch = {k: v.cpu().tolist() for k, v in batch.items()}\n",
    "        # ミニバッチのデータを事例単位のlistに変換する\n",
    "        predictions += convert_list_dict_to_dict_list(batch)\n",
    "    return predictions\n",
    "\n",
    "# ミニバッチの作成にDataLoaderを用いる\n",
    "validation_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "# 固有表現ラベルを予測する\n",
    "predictions = run_prediction(validation_dataloader, model)\n",
    "print(predictions[0][\"pred_label_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9e284-15c5-4d2e-aabc-4f9354452c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc910ea6-ee3a-49de-a9fb-23f01afc4fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0142e3-7551-4302-b9c5-35f955832778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a0124-9aa6-49a4-8880-44bb70bfd796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57586eda-83fa-4514-82cd-1651fcb18e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb9b57-a8a4-401f-8821-d38dc049bcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f6833-2f90-42ab-9405-2e051b17ccd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027b983-221e-4373-b56f-fbe35ef72386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
