{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac5acf64-e2c6-48f5-902e-69f9c0b246a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
      "Collecting openai\n",
      "  Downloading openai-1.35.2-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.3.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading openai-1.35.2-py3-none-any.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, tiktoken, openai\n",
      "Successfully installed distro-1.9.0 openai-1.35.2 tiktoken-0.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets openai tiktoken tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bca24dc7-1bde-4b2f-a763-e20c77db8328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from transformers.trainer_utils import set_seed\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "from pprint import pprint \n",
    "import random\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import BatchEncoding, pipeline\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers.utils import ModelOutput\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc691e5-4b69-493e-b645-44f725c19a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce375ddd-5d9f-47e8-8bcb-8c577e083451",
   "metadata": {},
   "source": [
    "# BPRの実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bab8e635-0408-4270-aec3-ab728f2d27f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの準備\n",
    "train_dataset = load_dataset(\"llm-book/aio-retriever\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9056b967-f80a-4923-8b0c-4e04d5fd5a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['qid', 'competition', 'timestamp', 'section', 'number', 'original_question', 'original_answer', 'original_additional_info', 'question', 'answers', 'passages', 'positive_passage_indices', 'negative_passage_indices'],\n",
      "    num_rows: 22335\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41c46acd-b85f-4ff3-ab3b-b6a014781c82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': ['骨川'],\n",
      " 'competition': 'abc ～the first～',\n",
      " 'negative_passage_indices': [0,\n",
      "                              1,\n",
      "                              2,\n",
      "                              3,\n",
      "                              4,\n",
      "                              5,\n",
      "                              6,\n",
      "                              7,\n",
      "                              8,\n",
      "                              9,\n",
      "                              10,\n",
      "                              13,\n",
      "                              14,\n",
      "                              15,\n",
      "                              17,\n",
      "                              18,\n",
      "                              19,\n",
      "                              21,\n",
      "                              22,\n",
      "                              23,\n",
      "                              24,\n",
      "                              25,\n",
      "                              26,\n",
      "                              27,\n",
      "                              28,\n",
      "                              29,\n",
      "                              30,\n",
      "                              31,\n",
      "                              33,\n",
      "                              34,\n",
      "                              36,\n",
      "                              37,\n",
      "                              38,\n",
      "                              39,\n",
      "                              40,\n",
      "                              41,\n",
      "                              42,\n",
      "                              43,\n",
      "                              44,\n",
      "                              45,\n",
      "                              46,\n",
      "                              47,\n",
      "                              48,\n",
      "                              49,\n",
      "                              50,\n",
      "                              51,\n",
      "                              52,\n",
      "                              53,\n",
      "                              54,\n",
      "                              55,\n",
      "                              56,\n",
      "                              57,\n",
      "                              58,\n",
      "                              59,\n",
      "                              60,\n",
      "                              61,\n",
      "                              62,\n",
      "                              63,\n",
      "                              64,\n",
      "                              65,\n",
      "                              66,\n",
      "                              67,\n",
      "                              68,\n",
      "                              69,\n",
      "                              70,\n",
      "                              71,\n",
      "                              72,\n",
      "                              73,\n",
      "                              74,\n",
      "                              75,\n",
      "                              76,\n",
      "                              77,\n",
      "                              78,\n",
      "                              79,\n",
      "                              80,\n",
      "                              81,\n",
      "                              82,\n",
      "                              83,\n",
      "                              84,\n",
      "                              85,\n",
      "                              86,\n",
      "                              87,\n",
      "                              88,\n",
      "                              89,\n",
      "                              90,\n",
      "                              91,\n",
      "                              92,\n",
      "                              93,\n",
      "                              94,\n",
      "                              95,\n",
      "                              96,\n",
      "                              97,\n",
      "                              98,\n",
      "                              99],\n",
      " 'number': '2',\n",
      " 'original_additional_info': '',\n",
      " 'original_answer': '骨川（滑川も正解）',\n",
      " 'original_question': '人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？',\n",
      " 'passages': [{'passage_id': 174162,\n",
      "               'text': 'ジャイアニズム(ジャイアン主義、剛田主義、剛田ニズム)とは、漫画およびアニメの『ドラえもん』に登場する主要キャラクターのひとりであるガキ大将、ジャイアンこと剛田武の作中における言動を、ジョーク的に思想と捉えるべく名づけられた名称である。',\n",
      "               'title': 'ジャイアニズム'},\n",
      "              {'passage_id': 1764440,\n",
      "               'text': '剛田 武(ごうだ '\n",
      "                       'たけし)は、藤子・F・不二雄の漫画作品『ドラえもん』に登場する架空の人物。通称「ジャイアン」。6月15日生まれ。野比のび太のクラスメイトのガキ大将。妹にジャイ子がいる。',\n",
      "               'title': '剛田武'},\n",
      "              {'passage_id': 76521,\n",
      "               'text': '『ドラミちゃん』のエピソードは、全8話のうち第1話「じゅん番入れかわりき」を除いた7話が『ドラえもん』の単行本に収録されているが、その際『ドラえもん』本編のストーリーとして改訂され、登場人物のほとんどが『ドラえもん』のものに差し替えられた。のび太郎は元々双方の母親が見間違うほどのび太に似ていたため、名前の表記を「のび太」に変えるだけで済んだが、その他の人物は主に顔が描き換えられ、のび太郎の母親であるのぶ子はのび太の母親である玉子に、ガールフレンドであるみよちゃんはしずかに、ガキ大将のカバ田はジャイアンになった。ただし、ズル木だけはスネ夫と体型が大きく違うためか、修正されずそのまま残されている。なお、アニメ版では役割そのものがスネ夫に差し替えられている。',\n",
      "               'title': 'ドラミ'},\n",
      "              {'passage_id': 861623,\n",
      "               'text': '「こんにちは、ぼくドラえもんです」(ドラえもん)、「ドラえもん、大変だよ」(のび太)、「のび太さんのエッチ!!」(しずか)、「のび太のくせに生意気だぞ!」(スネ夫)、「ひはははははは、ジャイアンこと剛田 '\n",
      "                       '武、小学五年生」(ジャイアン)などの台詞は全て原作には存在せず、それぞれの声優陣たちが自ら考案したアドリブである。他にもドラえもんがのび太を「のび太くん」、しずかがドラえもんを「ドラちゃん」、のび太らを「○○○さん」と呼ぶ設定と、「しずかちゃん」という愛称は大山のぶ代、野村道子が考案した。これらは第2作第2期にもほぼそのままの状態で残され、またさらなる発展が行われて現在に至っている。',\n",
      "               'title': 'ドラえもん (1979年のテレビアニメ)'},\n",
      "              {'passage_id': 627055,\n",
      "               'text': '作中では、空き地でこの道具を使って怪獣の映画の撮影をしたところをジャイアンとスネ夫に邪魔され、怒ったドラえもんとのび太は仕返しをするために彼らのぬいぐるみを作り、それぞれの家で悪事を働きそれぞれのママを怒らせ、空き地に逃げ本物のジャイアンとスネ夫に怒られる羽目となった(なお、ジャイアンとスネ夫のぬいぐるみを着たドラえもんとのび太は素顔を出して笑っていた)。',\n",
      "               'title': 'ドラえもんのひみつ道具 (ぬ-の)'},\n",
      "              {'passage_id': 1283696,\n",
      "               'text': '『がんばれ!ジャイアン!!』は、2001年3月10日に『ドラえもん '\n",
      "                       'のび太と翼の勇者たち』と同時上映公開されたドラえもんの映画作品である。26分20秒。『ドラえもん』の登場人物であるジャイアンこと剛田武の活躍を描いた物語。原作単行本第40巻「泣くなジャイ子よ」と第44巻「ジャイ子の新作まんが」をベースとしており、田中道明が作画を担当した漫画版も存在する。',\n",
      "               'title': 'がんばれ!ジャイアン!!'},\n",
      "              {'passage_id': 558296,\n",
      "               'text': 'その他の短編キャラクターでは神成さんが「のび太の大魔境」「のび太と竜の騎士」に登場し、空き地に起きっぱなしにされたドラえもんの道具に怒って破壊し、結果的に妨害する役割を果たしている。スネ夫のいとこスネ吉は直接は登場していないが「鉄人兵団」ではスネ夫のラジコンロボットのミクロス、「宇宙小戦争」ではスネ夫のプラモジオラマの製作者として言及がある。セワシやジャイ子は大長編に登場したことはない。主要メンバー以外は冒険に参加しないという事情のため、人間関係も短編とはやや異なる。例えば、短編ではジャイアンやスネ夫がのび太をバカにしたり仲間はずれにした(ここまでは大長編でも導入によく使われる)仕返しに、ドラえもんやのび太はジャイアンとスネ夫を仲間はずれにして、しずかや他の町の同級生たちと道具で遊ぶことがある。',\n",
      "               'title': '大長編ドラえもん'},\n",
      "              {'passage_id': 1713054,\n",
      "               'text': 'ドラえもん、のび太、しずかは専用のお助けアイテムが使え、ジャイアン、スネ夫は特殊能力が使える。ジャイアンとスネ夫はゲーム序盤、しずかはゲーム中盤から仲間に加わるが3人はアトランティスで一時的に離脱する。ドラミは宿屋で登場するが使用できない。未来のマフィア。時の宝玉を使って歴史を歪曲し自分たちの物にしようと企み、ドラえもんたちと対立する。',\n",
      "               'title': 'ドラえもん3 のび太と時の宝玉'},\n",
      "              {'passage_id': 1562155,\n",
      "               'text': 'しずか、ジャイアン、スネ夫は序盤では敵側に洗脳されボスのパートナーとして登場し(ボスの名前は、シズター(しずか)、ジャイコング(ジャイアン)、スネンガー(スネ夫)である。)、倒した後(救出する)にプレイヤーキャラクターとして使用できる。ドラえもんは4つ目のエリアで誘拐されるが、ボスを倒すと救出することができる。ドラミは終盤より使用可能となる。',\n",
      "               'title': 'ドラえもん3 のび太の町SOS!'},\n",
      "              {'passage_id': 874533,\n",
      "               'text': 'なお、ドラえもんはスイッチを押した本人ではないが消された人物のことを覚えており、のび太が「みんな消えてしまえ」と言ってスイッチを押したときにもドラえもんだけは消えていなかった。なお、当該人物が消えた後は「当該人物がいない世界」として歴史が改変され、当該人物のいた立場が別の人物に入れ替わっている。作中ではジャイアンを消した後、スネ夫がガキ大将になっており(第2作第1期では身長がのび太より高くなっている)、スネ夫を消した後は安雄や他の少年達がガキ大将になっていた。',\n",
      "               'title': 'ドラえもんのひみつ道具 (とあ-とこ)'},\n",
      "              {'passage_id': 174165,\n",
      "               'text': '「おまえのものはおれのもの、おれのものもおれのもの」というセリフは著名なもので、アニメでも決めゼリフのような扱いでしばしば使われているが、原作においてはこのセリフは一度しか登場せず、他には「タイムコピー」における「スネ夫の物はおれの物、おれの物もおれの物」があるのみである。ジャイアニズムという用語は俺様主義という言葉にも言い換えられることからも明らかなように、他にも『ドラえもん』の作中においてジャイアニズムの類型と言える決めゼリフが以下の通り存在する。(漫画を返してくれないと抗議され)「いつかえさなかった!? '\n",
      "                       'えいきゅうにかりておくだけだぞ。」この言葉は、元々はジャイアンのものではなく、初出時の「ドラミちゃん」の連載シリーズに登場していた「カバ田」というキャラクターのセリフである。容姿性格はジャイアンのジャイアニズムを踏襲している。',\n",
      "               'title': 'ジャイアニズム'},\n",
      "              {'passage_id': 3143056,\n",
      "               'text': 'のび太がお正月をのんびりと過ごしていると、突然、どこからともなく彼の未来を告げる声が聞こえ、タイムマシンに乗ってやってきたドラえもんと、のび太の孫の孫のセワシが現れた。セワシ曰く、落ちこぼれののび太は社会に出た後も沢山の不運に見舞われ、会社の倒産が原因で残った莫大な借金によって子孫を困らせているという。そんな悲惨な未来を変えるために、ドラえもんを子守用ロボットとしてのび太のもとへ連れてきたのだった。「ぼくにまかせれば大丈夫」、のび太にそう豪語するドラえもんだったが、ドラえもんの出したひみつ道具でかえってケガしてしまったのび太は、本当に大丈夫なのかと訝しむのだった。ドラえもんとのび太以外には同級生の源静香(しずか)、剛田武(ジャイアン)、骨川スネ夫が登場する。',\n",
      "               'title': 'ドラえもん'},\n",
      "              {'passage_id': 174140,\n",
      "               'text': '骨川 スネ夫(ほねかわ '\n",
      "                       'スネお)は、藤子・F・不二雄の漫画作品『ドラえもん』に登場する架空の人物。野比のび太のクラスメイト。2月生まれ(何日かは不明、午後5時5分に生まれたことがビデオに残されている)(方倉設定では3月28日生まれとされていた)。',\n",
      "               'title': '骨川スネ夫'},\n",
      "              {'passage_id': 2850436,\n",
      "               'text': 'テレビアニメの放送は30年以上にものぼっていることもあり、原作と比較して生活環境の描写が変わっていることも多い。例えば、2000年以降の作品ではスネ夫が携帯電話やDVDプレーヤーや携帯型ゲーム機を持っていたり、部屋にパソコンがあったり(出木杉も)、剛田雑貨店が自前のウェブサイトを開設していること、またテレビを操作する際にリモコンを使っていることなどが挙げられる。原作・アニメではファミリーコンピュータ発売以前にテレビゲームが登場し、スネ夫が自慢しているシーンが存在する。他、映画、がんばれ!ジャイアン!! '\n",
      "                       'のエンディングでのカレンダー表記が、2015年となっていたりするなど、若干の違いが見受けられる。 '\n",
      "                       '「ドラえもんの長い一日」においては、のび太が生活する現代は現在の設定では21世紀であるとされている。',\n",
      "               'title': 'ドラえもんの派生作品'},\n",
      "              {'passage_id': 1792386,\n",
      "               'text': 'グレードアップライトは、映画『ドラえもん '\n",
      "                       'のび太の宇宙英雄記』に登場する。これから出る光を当てると、どのようなものも本物のようになる。映画『ドラえもん '\n",
      "                       'のび太の宇宙英雄記』ではドラえもん、のび太、しずか、スネ夫、ジャイアンの着たヒーロースーツを本物のようにした。',\n",
      "               'title': '映画ドラえもんのひみつ道具'},\n",
      "              {'passage_id': 2137509,\n",
      "               'text': '※声優はドラえもん、のび太、しずか、ジャイアン、スネ夫以外は映画版とは異なる。',\n",
      "               'title': 'ドラえもん のび太の恐竜2006 DS'},\n",
      "              {'passage_id': 423146,\n",
      "               'text': 'TBS版の『オバケのQ太郎』でゴジラを演じた際、「スタジオの外だと面白いのに、中だとつまらない」とマネージャーから言われ、第4話収録時に開き直ってアドリブを入れまくったところ、それを見学に来ていた藤子が手を叩いて喜び気に入ったことがきっかけとなり、原作者指名で役を貰うこともあったほか、指名していない作品も制作時に藤子から「肝付さんは何をやるんですか」と聞くことがあったという。藤子作品で出演していない作品は『チンプイ』、『モジャ公』など。テレビ朝日版の『ドラえもん』では骨川スネ夫の声を1979年から2005年の3月の勇退するまで26年間担当したが、なお、肝付はこれ以前に、日本テレビ版の『ドラえもん』にてジャイアンこと剛田武役を担当していた。',\n",
      "               'title': '肝付兼太'},\n",
      "              {'passage_id': 962946,\n",
      "               'text': '無視虫(むしむし)は、「無視虫」(てんとう虫コミックス『ドラえもんプラス』第5巻に収録)に登場する。虫かごに虫型の道具が何匹も入っている。一見ノミのような形で誰かを無視している人にこの虫をとりつかせておくと、虫の付いた者が無視している相手に話しかけた際、この虫が体を刺す。涙目でひっくり返るほどの痛みがあるらしい。 '\n",
      "                       '作中ではジャイアンが、のび太をいじめた所を母親に目撃されて叱られたのを告げ口されたと邪推し、スネ夫の提案で「のび太を無視しろ」と周りの児童を煽動してのび太を孤立させた。それを知って大激怒したドラえもんは無視虫をジャイアンとスネ夫にとりつかせ、「のび太から嫌がらせを受けても、のび太を無視しなければならない」という強行手段を取った。',\n",
      "               'title': 'ドラえもんのひみつ道具 (む)'},\n",
      "              {'passage_id': 393065,\n",
      "               'text': '作中では、ドラえもんがお人好しののび太に飲ませたが、ジャイアンを執念深く追う(のび太がごまかされた額を計算違いして高く請求し、ジャイアンは支払いを拒んで逃げた)、ジャイアンの家に無理矢理上がり込もうとする、無関係のスネ夫達に乱暴する(体格上無理があるのに「ジャイアンが変装しているんだろ」「隠れているのはわかっている」とスネ夫の面皮を剥ごうとしたり静香のスカートをめくったりした)等、予想外な出来事が起こった。最終的にドラえもんがのび太に「これは毒だから飲むな」とウソを言い、それを疑ったのび太にスナオンを飲ませる事に成功した。',\n",
      "               'title': 'ドラえもんのひみつ道具 (きあ-きも)'},\n",
      "              {'passage_id': 2234343,\n",
      "               'text': 'ドラえもんはほとんど、のび太は全ての話に登場しており、次いで登場話数の多い人物(しずか、ジャイアン、スネ夫)とは登場話数で200話前後の差をつけている。しかし、テレビアニメではレギュラー5人の登場話数に差がつかないようになっている(原作で全く登場しない話でも、オリジナルの展開で登場することもある)。テレビアニメのスタッフロールでのキャスト紹介はドラえもんが最初に紹介されており、のび太は2番目。テレビアニメの字幕放送では、ドラえもんの台詞は主人公を表す黄色、のび太の台詞は準主人公を表す水色で表示される。',\n",
      "               'title': 'ドラえもんの登場人物一覧'},\n",
      "              {'passage_id': 4380663,\n",
      "               'text': 'ただし『のび太と鉄人兵団』は原作大長編の小説化となっているため、原作者逝去後のオリジナル映画を題材にしたのは本作が初となる。映画では描写されなかった設定やシーン、登場人物たちの心情などが描かれる。小説が描かれた代わりに、オリジナル版としては初めて、4コマ漫画を含め新作漫画が一切描かれなかった。本作の音楽を担当する服部隆之によるサウンドトラックが2018年2月28日に発売。全34曲収録。『STAND '\n",
      "                       'BY ME '\n",
      "                       'ドラえもん』(2014年)を除き、映画ドラえもんのシリーズでサウンドトラックが作品単独で発売されたのは『ドラえもん '\n",
      "                       'のび太の南海大冒険』(1998年)以来となる。『骨川スネ夫 '\n",
      "                       '初めてのラインライブ!コメント・ハート待ってます』のタイトルで2018年2月13日にドラえもん史上初となるLINE '\n",
      "                       'LIVEが行われた。出演はスネ夫・ジャイアン。また、2018年2月25日には『新作映画だよ!',\n",
      "               'title': '映画ドラえもん のび太の宝島'},\n",
      "              {'passage_id': 846549,\n",
      "               'text': 'なお例年の映画では冒頭でのび太が「ドラえも〜ん!」と叫ぶが、今作では珍しくスネ夫とジャイアンが叫ぶ。また長年監修をしてきた楠部大吉郎の最後の作品となった。音楽は前年の『南海大冒険』も手掛けた大江千里に加えて堀井勝美が参加。翌年からは堀井が単独で担当するようになり、テレビアニメ第2作第1期としては最終作となった『のび太のワンニャン時空伝』まで続投した。また、本作よりドルビーデジタル5.1chサラウンドが採用された。今作ではのび太たちとゲストキャラの友情(ドラえもんとログ、のび太とリアン、しずかとフレイヤ、ジャイアン・スネ夫とゴロゴロ)が描かれている。大長編ではジャイアンとスネ夫が冒険したいと言い、ドラえもん達がそれに反対するシーンがあるが、映画ではのび太とスネ夫の立場が逆になっており、ジャイアンがのび太の意見に賛同している。',\n",
      "               'title': 'ドラえもん のび太の宇宙漂流記'},\n",
      "              {'passage_id': 3066262,\n",
      "               'text': 'のび太とドラえもんはスネ夫とジャイアンに協力を要請し、スネ夫の案でザンダクロスの頭脳を仲間に加えようとひみつ道具「おはなしボックス」でひよこロボット、ピッポに改造し、鉄人兵団を迎え撃とうとする。',\n",
      "               'title': '映画ドラえもん 新・のび太と鉄人兵団 〜はばたけ 天使たち〜'},\n",
      "              {'passage_id': 846552,\n",
      "               'text': 'スネ夫に宇宙旅行のチケットを自慢されたのび太たち。のび太はジャイアンとしずか、そしてスネ夫とともにドラえもんに宇宙旅行を頼みに行く。「本当に宇宙に行けるわけない」と言うドラえもん。かわりに「スタークラッシュゲーム」という最新版の宇宙探検ゲームで遊ぶことになる。一方そのころ、UFOに乗ってやってきた謎の一行が地球に降り立つ。ジャイアンとスネ夫の2人が「スタークラッシュゲーム」の中で迷子になってしまい、やっとのことでゲームから脱出するとそこはUFOの中だった。既に「スタークラッシュゲーム」を終えていたドラえもん、のび太、しずかの3人はすぐに2人を探しに行くのだった。',\n",
      "               'title': 'ドラえもん のび太の宇宙漂流記'},\n",
      "              {'passage_id': 338403,\n",
      "               'text': 'ドラえもんの送り主ということもあり、頼りないドラえもんのフォローに回っている描写がある。ジャイアン・スネ夫・出木杉との交流も描かれており、各人を22世紀に連れていったりしている。',\n",
      "               'title': 'セワシ'},\n",
      "              {'passage_id': 3792126,\n",
      "               'text': 'スネ夫の家に遊びに行ったのび太は、スネ夫からフランスの最新高速列車TGVの鉄道模型を自慢される。のび太は家に帰り、悔し紛れにドラえもんに鉄道模型を出してくれと相談するが、ドラえもんは相手にしない。そこでのび太は藤子スタジオに電話をかけ、藤子不二雄の2人に「ぼくがヨーロッパの鉄道模型を買ってもらえる話を大至急作って下さい」とお願いする。藤本弘は困りはてるが、安孫子素雄は「作者が登場人物を甘やかしてどうするんだい。のび太くん、僕たちヨーロッパ旅行の準備で忙しいんだ」と突っぱねる。結局のび太は苦心して自分で模型を作るが、ジャイアンとしずかに見せようとしたところスネ夫に邪魔され、タケコプターで持ち上げたところをうっかり落としてあっさり壊れてしまう。',\n",
      "               'title': 'ドラえもん・ヨーロッパ鉄道の旅'},\n",
      "              {'passage_id': 542695,\n",
      "               'text': '夏休み。ドラえもんとのび太はひみつ道具・絵本入りこみぐつで絵本の中に入り『シンドバッドの冒険』を楽しんでいた。のび太は最近、絵本の世界に夢中になっており、ドラえもんが用事で出かけた後、しずかを誘いに行く。だが、彼女は今からピアノ教室の仲間とキャンプに行く為、代わりにジャイアンとスネ夫を誘うことになった。ジャイアンとスネ夫が『ジャックと豆の木』の世界に行っている最中、実は日程を1日勘違いしていたしずかも野比家を訪れる。2人は『ピノッキオ』の世界を楽しむことにしたが現実世界に一度戻ったジャイアンとスネ夫が絵本をごちゃ混ぜにしたことで無茶苦茶な話になってしまい、怒ったしずかは先に帰ってしまう。その直後、空中を高速で飛んできた何かに衝突し、しずかは気絶したまま落下してしまう。翌日、絵本入り込み靴が3足しか揃っていないことから誰か1人だけ現実世界に帰ってきていない可能性が出てきた。',\n",
      "               'title': 'ドラえもん のび太のドラビアンナイト'},\n",
      "              {'passage_id': 888825,\n",
      "               'text': 'スネ夫が、人気のミステリー列車の切符を3枚手に入れたと自慢する。ジャイアンとしずかは、ぜひ連れて行ってほしいとスネ夫に頼み込む。そこへのび太が駆け込むと、お約束通りスネ夫が「のび太の分はない」と嫌味を告げようとするも、のび太はどうでもいいと一蹴する。のび太は、何も言わずに3日間家を留守にしているドラえもんの行方を捜していたのだ。結局その日も当てはなく、すっかり落胆して帰るのび太だったが、家に帰ると何事もなかったかのようにドラえもんがいた。彼の話によると、別の用事で22世紀へ戻った際、長い行列を見て並んでみたら、それは22世紀で大人気の銀河ミステリー列車の切符の販売前で、3日間かかってやっと手に入れたらしい。それを知って大喜びするのび太。2人はさっそくその話題のミステリー列車に乗り込む。それは列車というより宇宙船で、さながら銀河鉄道の夜の世界であった。',\n",
      "               'title': 'ドラえもん のび太と銀河超特急'},\n",
      "              {'passage_id': 348839,\n",
      "               'text': 'チューシン倉(チューシンぐら)は、「チューシン倉でかたきうち」(てんとう虫コミックス『ドラえもんプラス』6巻に収録)に登場する。倉を象った小さな道具。この倉の中に、憎い相手の名前を書いた紙を入れると、赤い紙が出てくる。その赤い紙を誰かに渡すと、その渡された者が憎い相手に復讐する。赤い紙を渡された者は強制的に復讐をしなければならないが、渡された者や憎い相手によって時間が掛かる場合もある(作中では赤い紙を渡されたスネ夫がジャイアンに復讐しようとした際、ジャイアンに怯えてうずくまっていた)。復讐を行う最中に別の憎い相手を変更することも可能である。作中ではのび太はこの道具を使って、自分を何回も叩いておきながら一回殴り返された事だけママに告げ口したスネ夫(復讐した者はジャイアン)、その事情も聞かずに一方的にのび太のせいにした玉子(復讐した者はドラえもん)に復讐した。',\n",
      "               'title': 'ドラえもんのひみつ道具 (ち)'},\n",
      "              {'passage_id': 627057,\n",
      "               'text': 'その後、悔しがったのび太にドラえもんがジャイアンママのぬいぐるみを用意しジャイアンママに成りきり、ジャイアンとスネ夫にお仕置きを行い復讐した。',\n",
      "               'title': 'ドラえもんのひみつ道具 (ぬ-の)'},\n",
      "              {'passage_id': 3309217,\n",
      "               'text': 'そもそもの原因であるのび太はスネ夫たちに無理矢理キスをさせられ、目覚めたジャイアンに惚れられてしまった。これを見たドラえもんは「おとぎ話のようにハッピーエンドにはならなかったよ」とあきれていた。',\n",
      "               'title': 'ドラえもんのひみつ道具 (1979年のテレビアニメ さ-そ)'},\n",
      "              {'passage_id': 1315453,\n",
      "               'text': '本作の声優を担当した声優の一部は、シンエイ版にも続投して主要人物を演じており、太田淑子はのび太からセワシ、小原乃梨子はのび太のママからのび太、肝付兼太はジャイアンからスネ夫を演じている。また我成先生(のび太のクラス担任)、スネ夫の父を担当した加藤治は同じく先生は一時期、スネ夫の父は2005年に声優陣が一新されるまで担当していた(途中、代役あり)。劇場版では、富田耕生は『ドラえもん '\n",
      "                       'のび太の海底鬼岩城』でポセイドン、『ドラえもん のび太の南海大冒険』でドクタークロン、『ザ☆ドラえもんズ '\n",
      "                       'おかしなお菓子なオカシナナ?』でサト国王を演じているほか、野沢雅子は『ドラえもん '\n",
      "                       'のび太の宇宙漂流記』でログを、『ドラえもん のび太とロボット王国』でクルリンパを、『ドラえもん '\n",
      "                       'のび太と奇跡の島 〜アニマル アドベンチャー〜』で少年のび助を演じた。',\n",
      "               'title': 'ドラえもん (1973年のテレビアニメ)'},\n",
      "              {'passage_id': 558281,\n",
      "               'text': '『大長編ドラえもん』(だいちょうへんドラえもん)は、藤子・F・不二雄による日本の児童・SF漫画作品。長編アニメ映画の原作として1980年(昭和55年)より毎年1編が執筆された『ドラえもん』の長編作品で、映画公開に先行して『月刊コロコロコミック』で連載された。通常の『ドラえもん』が掲載1回毎の完結を基本としているのに対し、『大長編』は映画1作の原作となる1つの長編が数回に分けて連載され、ドラえもん・野比のび太・源静香・剛田武(ジャイアン)・骨川スネ夫の5人が編毎に異なる様々な冒険に立ち向かう様が描かれる。単行本も『ドラえもん』から独立した『大長編ドラえもん』として発行されている。1996年(平成8年)の藤子Fの死後も藤子・F・不二雄プロが2004年(平成16年)まで続編を制作し、単行本も続巻として発行された。',\n",
      "               'title': '大長編ドラえもん'},\n",
      "              {'passage_id': 831481,\n",
      "               'text': '第2エリア以降は無限ループも存在し、正しいルートを選ばないと先に進めない(ループの回数によるペナルティはないが、隠し部屋が途中にある場合は1度だけの利用になる)。またこのワールドでは、アイテム入手による得点の加算が一切ない。初期状態の武器はショックガン(4連射が可能)だが「スモールライト」を入手すると広範囲で敵への攻撃が可能となり、しかもボス以外なら一撃で倒すことができる(スモールライトは開拓編でも入手可能で、その場合は魔境編のスタート時の装備がスモールライトになる)。またスネ夫やジャイアンを助けると彼らはドラえもんと共に行動し、スネ夫は上下(左右)斜め2方向ショックガンで、ジャイアンは投石で援護攻撃をする(攻撃の強さはドラえもんの所持武器に準ずる)。ドラえもんの所持武器がスモールライトであればスネ夫・ジャイアン両共敵を一撃で倒せる。',\n",
      "               'title': 'ドラえもん (ファミコン)'},\n",
      "              {'passage_id': 541790,\n",
      "               'text': 'その為、のび太たちは物語中盤まで(大長編においては珍しく)主人公であるドラえもん不在での冒険を余儀なくされ、のび太としずか、ジャイアンとスネ夫がそれぞれ二手に別れて別行動をとる。のび太とスネ夫が、それぞれ様々な困難を解決に導き出す役割を果たす。この作品から野比のび助役の声優が加藤正之から中庸助に交代した。のび助は作品冒頭に登場し、物語の発端となる。',\n",
      "               'title': 'ドラえもん のび太とブリキの迷宮'},\n",
      "              {'passage_id': 2070544,\n",
      "               'text': 'なお、ジャイアンとスネ夫はのび太に近づく為に2冊の漫画(それぞれタイトルの頭文字が「ジャ」と「ス」)を持ってきたが、テレビアニメ第2作第1期では「ジャムパン」と「スケートボード」になっている。また、テレビアニメ第2作第2期では「ゴロゴロコミック」と「星野(ほしの)スミレのサイン入り色紙」になっているが、これは2人の名字である剛田(ごうだ)と骨川(ほねかわ)を利用した作戦である。',\n",
      "               'title': 'ドラえもんのひみつ道具 (はな-はん)'},\n",
      "              {'passage_id': 1202729,\n",
      "               'text': '悔しさと怒りのあまり、自分を心配して様子を見に来たしずかを意に介さず、落としたどら焼きも置いて逃げ帰ったのび太はドラえもんが残した道具・ウソ800を使い、言ったことを嘘にすることでジャイアンとスネ夫に仕返しするが急に空しくなり、2人を許す。帰宅したのび太はママに「ドラちゃんには会えたの?」と問われて「ドラえもんは帰ってこない」と返答する。自室に戻ったのび太の目に映ったのは本物のドラえもんであった。ウソ800の効力が残っていた為、ドラえもんは帰ってこないことが嘘になったのだ。のび太が落としたどら焼きを手に野比家に足を運んだしずかとジャイアンとスネ夫もドラえもんと再会を果たし、皆、和解するのだった。',\n",
      "               'title': '帰ってきたドラえもん'},\n",
      "              {'passage_id': 1792446,\n",
      "               'text': '発信機(はっしんき)は、映画『ドラえもん '\n",
      "                       'のび太のひみつ道具博物館』に登場する。虫型の発信機。羽根の色がそれぞれ違っていて、ドラえもんは水色、のび太は赤、スネ夫は黄緑、ジャイアンはオレンジ、しずかは桃色、クルトは白。発信機の地図で誰がどこにいるか、色別で分かるようになっている。',\n",
      "               'title': '映画ドラえもんのひみつ道具'},\n",
      "              {'passage_id': 542620,\n",
      "               'text': 'なお、原作ではスネ夫とジャイアンが先生に怒られるオチだったが、アニメでは本物の宇宙人が現れるオチとなった(この時のび太たちは終始ドラえもんの道具だと勘違いしていた)。',\n",
      "               'title': 'ドラえもんのひみつ道具 (くな-くん)'},\n",
      "              {'passage_id': 1366386,\n",
      "               'text': 'ジャイアンこと剛田武のセリフである「おまえのものはおれのもの、おれのものもおれのもの」も知られており、また漫画本以外の関連グッズも人気があるという。キムドン社の社長であるファン・クァン・ビンは、『ドラえもん』が人気となった高度経済成長期の日本と、1990年代以降のベトナムは時代背景が重なっていることが人気の理由ではないかと述べている。1999年からはテレビアニメの放映も開始されている。テレビアニメは10年以上に渡って放送されており、オープニング主題歌である『ドラえもんのうた』はベトナム人に最もよく知られた日本の歌の一つとされる。2006年には映画シリーズの『ドラえもん '\n",
      "                       'のび太の恐竜2006』が上映。',\n",
      "               'title': 'ベトナムにおけるドラえもん'},\n",
      "              {'passage_id': 602078,\n",
      "               'text': '一時的人気者」「矢口のラジオ 笑いと言い訳」「キダムが来ます '\n",
      "                       '2人だけ来ます」など。前述、山川のネタの約8割がドラえもんのネタだったため、山川からリニューアルしたもの。「ドラえもんはなんで僕の家にいるの?...罰ゲーム」「ドラえも〜ん。ジャイアンにいじめられたよ〜...お前はそれでギャラもらっているんだからピーピー言うなっ!」「ドラえも〜ん。ジャイアンにいじめられたよ〜...俺がやれ!って言ったんだよ」「ドラえも〜ん。スネ夫がハワイに行くんだよ〜...じゃ、俺が三鷹に連れてってやる」「ドラえも〜ん。ポケットの中見せてよ〜...おっ?何だ?セクハラかぁ?」「ねえ、ドラえもんの大好物は何?...熟女」など。普段リスナーが面と向かって言えないことを矢口が代わりに言う、という感動コーナー。上記コーナーのほかに、ハロー!プロジェクトメンバーによる内包番組を放送していた。',\n",
      "               'title': 'あなたがいるから、矢口真里'},\n",
      "              {'passage_id': 859146,\n",
      "               'text': 'そのまま静香達の行動を撮影したが、その夜に静香達(スネ夫は行かなかった)がのび太の家に訪れ、フィルムを見せてくれとせがまれ、ドラえもん達は見せるかどうか困っていた。原作では静香達がフィルムを見ている隙にドラえもん達がこっそり逃げようとする所で話が終わったが、テレビアニメ第2作第1期ではフィルムを見た静香達が激怒し、のび太達を訴える所で話が終わっている。リメイク版ではレンズをスネ夫の飼い猫・チルチルに付けており、菓子作りに失敗して泣く静香、少女漫画を見て泣くジャイアン、チルチルに驚いてお漏らしをするスネ夫、キャッチボールで神成さんの窓ガラスを割って逃げる安雄、塾をサボって本屋で立ち読みをするはる夫を撮影した。',\n",
      "               'title': 'ドラえもんのひみつ道具 (み)'},\n",
      "              {'passage_id': 1002801,\n",
      "               'text': 'また、のび太がジャイアンとスネ夫に追いかけられているのを知らずに、ジャイアンとスネ夫が家の前で待ち構えているのにも関わらず問答無用で強制的にお使いに行かせようとしていたり、のび太を殴ろうと待ち伏せしているジャイアンとスネ夫を恐れて学校に行こうとしないのび太を強引に学校に行かせようとしていた。また、学校から少し帰ってくるのが遅くなっただけで怒鳴りつけた事もある。先生からの激励「目は何の為に前についていると思う? '\n",
      "                       '前進するためだ! '\n",
      "                       '終わったことをいつまでも気に病むな。次回(のテストで)頑張れ」を紹介しようとしたのび太を「屁理屈ばっかり!」と怒鳴りつけた事もある。のび太の失敗に対しては、ジャイアンとスネ夫にいじめられたり不可抗力がある場合でも事情や理由も聞かず頭ごなしに叱るなど、息子に対する信頼は皆無で、のび太よりもスネ夫のおべっかを信用する場面も見られる。',\n",
      "               'title': '野比玉子'},\n",
      "              {'passage_id': 768767,\n",
      "               'text': '秋のある日、恐竜が今でも生き残っていると言い張り、例によってジャイアンとスネ夫に笑い者にされたのび太は、ドラえもんのひみつ道具「○×うらない」でも「地球上に生き残っている恐竜はいない」と判定され落胆する。ところが直後、多奈川で巨大な生物を目撃したスネ夫は、それが恐竜ではないかという疑念にかられてすっかり動転してしまい、挙げ句の果てにノイローゼになってしまう。一方、のび太は0点の答案を隠すためにひみつ道具の「どこでもホール」を使い、地底にある大空洞を発見する。ドラえもんとのび太はしずかやジャイアンを誘って地底の大空洞を秘密の遊び場にするが、独り、恐竜の幻の正体を突き止めようと単独行動していたスネ夫が地底で迷子になり、その直後に「どこでもホール」が壊れてしまう。',\n",
      "               'title': 'ドラえもん のび太と竜の騎士'},\n",
      "              {'passage_id': 1070455,\n",
      "               'text': 'ゾウ印口べに(ゾウじるしくちべに)は、「ゾウ印口べに」(てんとう虫コミックス『ドラえもんプラス』第1巻に収録)に登場する。この口紅を唇に塗って心で「伸びろ」と念じると、上唇がゾウの鼻のように伸び、人の体くらいはかるがると持ち上げることができる。原作では静香がジャイアンとスネ夫によるのび太へのいじめに耐えかねて使ったが、テレビアニメ第2作第2期「ゾウの鼻になったしずかちゃん」では「ゾウ印仮面」というヒロインに扮して登場。ジャイアンとスネ夫に限らず、町中に蔓延る全ての悪人を退治した。類似品に「タコ印口べに」があり、相手に墨を吐いてしまう。',\n",
      "               'title': 'ドラえもんのひみつ道具 (そ)'},\n",
      "              {'passage_id': 859135,\n",
      "               'text': 'ミニ野球場とミニプレイヤー(ミニやきゅうじょうとミニプレイヤー)は、『ドラえもん '\n",
      "                       'のび太の宇宙開拓史』に登場する。自動的に野球の試合をする玩具。プレーヤーの人形の鼻を押すと、押した人に見た目が似、プレーヤーの能力値までも本人に似る。作中で実際に使用されたことはなく(ドラえもんによる試用を除く)、ジャイアンとスネ夫が預かっていっただけである。',\n",
      "               'title': 'ドラえもんのひみつ道具 (み)'},\n",
      "              {'passage_id': 104492,\n",
      "               'text': '自分の思い通りにならないと感情的になり、両親やドラえもんに叱られたときに声を荒らげてしまい、ドラえもんと喧嘩することもある。また、スネ夫とジャイアン、大好きなしずか(※ただし、アニメ版ではしずかに対して声を荒らげるシーンの描写は少ない)に対しても声を荒らげる場合もある。しずかにとんでもないイタズラをし、面白がることもある。その場の空気を読まないことが多く、見当はずれなことを言ってしまい、ムードを盛り下げてしまうことがある。また、初対面のおばさんに対して(ひみつ道具の影響もあるものの)サルと言ったり、草花を植えたジャイアンに対し、みんなが彼を誉める中、のび太は「がらにもないこと」といい、ジャイアンを怒らせてしまったこともある(ほうほうのていで逃げ出した後に、「ぼくはつい本当の事を言う」とボヤいている)。ジャイアンやスネ夫に濡れ衣を着せられることも多い。',\n",
      "               'title': '野比のび太'},\n",
      "              {'passage_id': 1363378,\n",
      "               'text': '台風一過の朝、のび太は台風の子供と出会い、フー子と名付けて可愛がる。翌日、ドラえもん、のび太、しずか、フー子の4人は、どこでもドアで大草原に遊びに行き、そこで未開の地である風の村に行き着き、風の民であるテムジンたちと仲良くなった。スネ夫はフー子が自分の家の庭で産まれたことから、自分のものにしようとし、フーコに酷い目に遭わされたジャイアンを味方に付けて、やってくるが、ジャイアンはフー子と和解してしまう。帰宅するドラえもん一行であったが、フー子を諦めきれないスネ夫に風の民と対立している嵐族の古代の長・ウランダーが憑依。乗っ取られたスネ夫は嵐族と共にフー子を奪おうとする。',\n",
      "               'title': 'ドラえもん のび太とふしぎ風使い'},\n",
      "              {'passage_id': 680428,\n",
      "               'text': '『ガリヴァー旅行記』のリリパット国冒険譚をモチーフとし、またタイトルの由来になったアメリカ映画『スター・ウォーズ』に対するオマージュ要素、加えて古典SF『縮みゆく人間』も発想のヒントとなっており、同作で小さくなった主人公が玩具の家で生活するシーンなどが、本作に影響を及ぼしている。なお、作者は以前に前述の小説、映画のオマージュである短編「天井裏の宇宙戦争」(てんとう虫コミックス19巻収録)を発表していた。『大長編ドラえもん』で唯一、連載時最終回の後半部分を袋とじにする趣向が用いられている。これは読者の結末への想像を膨らませ、注目させるのが狙いだった。大長編では「ドラえもん・のび太・しずか」と「ジャイアン・スネ夫」というメンバー構成が多いが、本作では「ドラえもん・のび太・ジャイアン」と「しずか・スネ夫」のチームで行動している。特にスネ夫の活躍が顕著であり、物語のキーパーソンとなる。',\n",
      "               'title': 'ドラえもん のび太の宇宙小戦争'},\n",
      "              {'passage_id': 1895802,\n",
      "               'text': 'お天気コーカン地図(おてんきコーカンちず)は、「お天気コーカン地図」(2003年5月23日放送、映像ソフト未収録)に登場する。付いているコマを世界中の国の場所に置くと、その国の天気と書いた場所の天気を入れ替えることができる。作中では、のび太、ジャイアン、スネ夫たちがサッカーの試合をする日に雨がふっていたため、地図で天気を入れ替えた。家に地図が置いてあるのを発見したのび太のママがいろんな国の場所にコマを移動させたため、のび太、ジャイアン、スネ夫のチームや観戦に来ていたドラえもんとしずか、相手チームのメンバーが大風や吹雪、嵐や竜巻などに巻き込まれた。',\n",
      "               'title': 'ドラえもんのひみつ道具 (1979年のテレビアニメ う-お)'},\n",
      "              {'passage_id': 2887002,\n",
      "               'text': 'ジャイアンとスネ夫がバカみたいに釣れるという池を見つけたとはしゃいでいた。ドラえもんとのび太は自分達も行こうと準備をするが、母親から危ないところに行ってはいけないと咎められてしまう。そこでドラえもんは、ひみつ道具『お座敷釣堀』を使ってのび太の部屋で釣りをすることを考えるのだった。',\n",
      "               'title': 'ドラえもん 勉強べやのつりぼり'},\n",
      "              {'passage_id': 1202728,\n",
      "               'text': 'ボロボロの身ながらも自分だけの力でジャイアンに勝ったことを誇らしげに語るのび太に涙するドラえもんはのび太を連れ帰り、眠ったのを見届けた後、未来の世界に帰還する。のび太はドラえもんがいない寂しさを噛みしめながら、ドラえもんはのび太との最後の思い出の場所である公園跡地に毎日のように足を運びながら、日々を過ごす。それでも2人はしずかやドラミたちの気遣いもあり、前を向こうとしていた。そんなある日、のび太はジャイアンから「ドラえもんを見かけた」と教えられる。どら焼きを買い、ドラえもんを探し回るのび太だが、そのドラえもんはスネ夫が化けた偽物であり、全てはジャイアンがエイプリルフールにかこつけて、先日のケンカの仕返しに行ったことだった。',\n",
      "               'title': '帰ってきたドラえもん'},\n",
      "              {'passage_id': 1764453,\n",
      "               'text': 'また、テレビアニメ第2作第1期の「本音ロボット」では、内心ではのび太のことを友達だと思っているが、なかなか素直になれずに暴力を振ってしまうと本音ロボットを介して語っている。入学式の日、のび太が失くしてしまった自分のランドセルをずぶ濡れになりながら探し、ランドセルがあったトラックを必死で追いかけたことがある。レギュラーメンバーの中ではスネ夫と行動を共にすることが多く、スネ夫との友情を感じさせる描写も少なくない。しかしスネ夫は内心ジャイアンに対してかなり鬱憤が溜まっているようで、のび太やドラえもんと一緒に仕返しをすることも多い。クラスメイトには横暴な振る舞いが多いが、出木杉には個人的な相談に乗ってもらうなど、一目置いている。ただし回によっては暴力を振るったり、漫画を奪ったりするなど、出木杉自身はジャイアンを恐れている様子。恋愛面に関しては苦手な所がある。',\n",
      "               'title': '剛田武'},\n",
      "              {'passage_id': 689855,\n",
      "               'text': 'そしてドラえもんが現れ、「無茶な使い方をしたからテープが切れてもう夢から抜け出せない」と諭されて過ぎ去ってしまうと、前に見た夢の登場人物に報復され、最後に実はその全てが「夢だからと言って何でも思い通りになるとは思わず、何事も目標を定めて努力を怠らないこと」を説く、教訓の夢の一環であったと明かされる一種のトリックとなっていた。テレビアニメ第2作第1期ではこれらに加え、SFの夢(実際は教訓の夢の一部)として『スター・ウォーズ』のパロディの夢も登場した。のび太がルーク・スカイウォーカーに扮した「ノービ・ノビウォーカー」になり、しずかはレイア姫に、ドラえもんとジャイアンとスネ夫はチューバッカ、R2-D2、C-3POにそれぞれ扮した。挙句の果てに悪役は先生が扮したダース・ベイダーならぬ、「ダース・ベンキョー」であった。',\n",
      "               'title': 'ドラえもんのひみつ道具 (とさ-とん)'},\n",
      "              {'passage_id': 831482,\n",
      "               'text': 'なお、スネ夫・ジャイアン両共オプション扱いとなっており壁や敵に対して無敵となっている。このワールドでの被ダメージ時の無敵時間はドラえもんのみ。ダメージ判定は受けた瞬間で、ライフが減るのは無敵時間終了後のため、無敵時間中に回復アイテムを取得すると終了後にライフが減ることになる。ダメージ判定とライフ減少にタイムラグ(=時間差)がある為ライフ減少により0以下でミスとなる直前にライフを回復した場合はミス判定を取られずそのままゲーム続行できる。ドラえもんが4回ダメージを受ける(壁の接触も含む)とスネ夫もしくはジャイアンが離脱したり、アイテムを失ったりする。なお複数アイテムを所持している場合はいずれか1つを失いミスを除き全て同時に失うことはない、紛失優先順位はヒラリマント>スモールライト>スネ夫=ジャイアンの順。こちらは4回ダメージを受けた時点で即座に離脱・紛失する。',\n",
      "               'title': 'ドラえもん (ファミコン)'},\n",
      "              {'passage_id': 962957,\n",
      "               'text': '無敵砲台(むてきほうだい)は、「スネ夫の無敵砲台」(てんとう虫コミックス第38巻に収録)に登場する。ドラえもんがポケットから出したものではなく、ジャイアンに泣かされるスネ夫が、のび太から未来デパートのカタログを譲ってもらって買ったもの。部屋ほどの大きさもある砲台で、どこかにセットしておき、誰かを指差して「発射!」と合図すると、思いのままにいつでも誰でも砲撃できる。威力は相手が黒焦げになって倒れるほどだが、砲撃から逃れることも、砲撃を防ぐことも一切不可能。のび太がスネ夫に買ってあげたことが判明した際、ドラえもんがひっくり返って驚くほどの恐ろしい兵器である。砲台の機能を停止できるのはセットした本人のみ。本人以外が停止させるには砲台を破壊するしか手段はないが、砲台に接近した者はレーダーによって探知され、たちどころに迎撃されるため、破壊は殆ど不可能と言える。',\n",
      "               'title': 'ドラえもんのひみつ道具 (む)'},\n",
      "              {'passage_id': 882617,\n",
      "               'text': 'なんでも空港(なんでもくうこう)は、「なんでも空港」(てんとう虫コミックス32巻に収録)に登場する。紙工作の空港のような道具で、折りたたまれている滑走路を数メートルほどに広げて使う。これを地面に広げておくと、空を飛んでいるものは生物から無生物に至るまで(ジャンボジェットのような巨大な飛行物体でも)ひとりでにこの空港へ降りてくる。元通りに空港を折畳むと効果はなくなる。昆虫を集めようとしてジャイアンとスネ夫が使用した際には、空から藤子不二雄両作品のキャラクターが多数登場し、「日本の空には、へんなものが飛んでるんだなあ」と二人を呆れさせた(テレビアニメ第2作第2期「なんでも空港」ではドラえもんたちが使用した際に藤子不二雄両作品のキャラクターが多数登場(前述のジャイアンとスネ夫の台詞も「日本の空にはいろいろなものが飛んでるんだなあ」というドラえもんの台詞に変更)。',\n",
      "               'title': 'ドラえもんのひみつ道具 (な)'},\n",
      "              {'passage_id': 871486,\n",
      "               'text': 'イメージライトキャップは、「スネ夫は理想のお兄さん」(てんとう虫コミックス第40巻に収録)に登場する。ライト付きのサンバイザーのような道具で、頭にかぶって使用する。この道具で照らされた相手は、かぶっている者のイメージ通りに行動する。キャップをかぶっている人から離れると、効果は消える。例として作中では、スネ夫が弟のスネツグに、自分は優等生で女の子にモテモテで乱暴者のジャイアンも自分にはへつらう、と偽っていたが、スネツグがこれをかぶることで、先生がスネ夫を褒めたり、女の子たちがスネ夫を見てキャーキャー言ったり、ジャイアンがスネ夫にペコペコへつらう、といった効果が現れた。',\n",
      "               'title': 'ドラえもんのひみつ道具 (いな-いん)'},\n",
      "              {'passage_id': 615077,\n",
      "               'text': 'このときスネ夫のみ台詞があり、カマイタチのことを話している。またアニメ版では『ドラえもん』のBGMが流れている。同じく、「ずっこけお正月」でも近所の子供としてジャイアンとスネ夫が登場しているが、こちらはアニメ化されていないエピソードである。2007年3月10日公開の映画『ドラえもん '\n",
      "                       'のび太の新魔界大冒険 '\n",
      "                       '〜7人の魔法使い〜』において、作中のテレビアニメ番組として「魔法少女マミ」(魔法の世界では「科学少女マミ」)が描かれ、その主人公として魔美をベースとしたキャラクターが登場した。マミの声を演じたのは瀬那歩美。明確に魔美をモチーフとしたキャラクターが藤子アニメに登場したのは、本編が終了して以来18年ぶりのこと。2010年11月19日放送のアニメ、『ドラえもん』「あやとり世界の王様に」においてモブキャラクターとして登場。クラスメートのノンちゃんといっしょにのび太の活躍に賞賛の拍手を送っていた。',\n",
      "               'title': 'エスパー魔美'},\n",
      "              {'passage_id': 2574187,\n",
      "               'text': '本作のキャラクターデザインは原作第30巻前後に近いデザインになった(具体的には、ドラえもんの頭がやや大きくなった。しずかの髪の色がこれまでの淡い栗色から黒髪に(ただし、『ドラえもん '\n",
      "                       'のび太の新魔界大冒険 '\n",
      "                       '〜7人の魔法使い〜』では、濃い栗色になっている)変更されている。ドラえもん、のび太、しずか、ジャイアン、スネ夫の黒目が大きくなってハイライトが入る=従来の●から○へ。しずかの目を閉じる絵が睫毛と一緒に下がるものに変更され、通常時のジャイアンの白目の表現がなくなった。しずかの母の顔のデザインは原作とは違い従来のアニメのまま。キャラクターの服装のデザインは各話ごとに変わっているが、のび太、ジャイアン、スネ夫は第2作第1期の普段着と同じデザインであることが多い(のび太の服の色は朱色、水色の場合もある)。',\n",
      "               'title': 'ドラえもん (2005年のテレビアニメ)'},\n",
      "              {'passage_id': 1258069,\n",
      "               'text': 'その夜の剛田商店ではジャイアン、スネ夫、出木杉によるのび太のためのバチェラー・パーティーが行われ、その帰り道、のび太は小学校時代の先生と会う。先生を気遣って、上着をかけたのび太に先生は「明日は遅刻するんじゃないぞ」と激励を送る。しずかは両親とのお別れパーティーを行うがドラえもんとのび太が源家に足を運んだ直後、しずかは父親に「私が結婚したら、パパやママが寂しくなってしまうから結婚をやめる」と言い出す。そんなしずかに父はしずかが生まれて来てからの楽しい日々を語り、その思い出があるから大丈夫なことに加え、のび太の人間性も説き、しずかを安心させる。現代に帰還したのび太はしずかに「必ず、君を幸せにする」と約束するのだった。',\n",
      "               'title': 'のび太の結婚前夜'},\n",
      "              {'passage_id': 873770,\n",
      "               'text': 'ケロンパスは、「ケロンパス」(てんとう虫コミックス『ドラえもんプラス』6巻に収録)に登場する。湿布を模した道具で、使用法も湿布と同じ。身体に貼ると疲れを全て吸い取ってくれる。1枚だけで何度でも(何人分でも)疲れを吸い取る事が可能であり、ドラえもんはこの道具を使って36人分の疲れを取った。吸い取った疲れを人に移す事も出来る。作中ではドラえもんがのび太に前述の36人分の疲れを一気に移したため、あまりの疲れでのび太は倒れて動けなくなってしまった。テレビアニメ第2作第2期では吸い取れる疲れの量に限界があるとされており、ドラえもんは36人分の疲れを吸い取ったところで「もういっぱいだ」と述べている(ただしその後もジャイアンとスネ夫の疲れを吸い取っている)。',\n",
      "               'title': 'ドラえもんのひみつ道具 (け)'},\n",
      "              {'passage_id': 850305,\n",
      "               'text': '高高度から落下したのび太を救うため、ドラえもんが雲固めスプレーを使用して作った雲の大地である。そこへ他のひみつ道具を用いて作った王国。1株100円の株式制を採用した「株式国家」。総工費のうちほとんどをスネ夫が出した(スネ夫・3万円=300株につき大株主。静香は100円で1株、ジャイアンは50円で半株)。小さな国であるが、山、谷、川、滝、湖など基礎的な自然物があり、レストラン、図書館、ゲームセンター、野球場、テニスコートなどの施設が充実している。労働力・遊び相手として、雲の切れ端にロボッターを入れて作った雲ロボットが多数いる。役職としては、のび太が国王、しずかが王妃、ドラえもんが総理大臣、スネ夫がナンデモ大臣、ジャイアンが召し使い、となっているがほぼ肩書だけである。',\n",
      "               'title': 'ドラえもん のび太と雲の王国'},\n",
      "              {'passage_id': 873762,\n",
      "               'text': '原作ではのび太と出木杉が、テレビアニメ第2作第1期ではジャイアンとスネ夫も加えて参加した。どんなゲームも簡単過ぎてすぐに飽きるという出木杉は見事にクリアしたが、考え無しに行動したのび太(テレビアニメ第2作第1期ではジャイアンとスネ夫も)は序盤で早くも詰んでしまう。出木杉に負けたくなかったのび太は予め持ち込んでいた四次元ポケットからどこでもドアを使用して無理矢理クリアにこぎつけたが、のび太(テレビアニメ第2作第1期ではジャイアンとスネ夫も)は反則した為、落石の下敷きでペラペラ状態となる羽目になった。ドラえもんも実際にクリアしてドラ焼きを手に入れている。テレビアニメ第2作第2期では内容が大幅に変更され、冒険の大部分がカットされた代わりに子供のスフィンクスが出題するなぞなぞに答える関門が追加されている(本来のスフィンクスは父親という形で登場)。',\n",
      "               'title': 'ドラえもんのひみつ道具 (け)'},\n",
      "              {'passage_id': 174151,\n",
      "               'text': '大長編においては、通常回ののび太以上に弱音を吐くことが多いが、仲間たちと共に苦難に立ち向かっていき、ドラえもんのように知恵を絞ったり、得意のラジコン操作を活かす場面が描かれることも少なくない。のび太がドラえもんに道具を出してくれるよう懇願する発端の多くはスネ夫の自慢話であり、この自慢癖は母親譲りの様子。ジャイアンの嫌がらせが身体的暴力が中心であるのに対し、スネ夫は「のび太だけ除外」などという精神的な嫌がらせが中心。のび太にだけゲームを貸さない、コレクションに触らせない、遊びや行楽、別荘(四丈半島)に誘わないといった類が多い。主に「この車にはあと3人しか乗れない」「チケットが3枚しかない」など。44巻収録「アニメばこ」には「3人用のビデオ」という理不尽なものが登場する。しかし、あまり度が過ぎると、のび太だけでなくジャイアンやしずかまでも呆れさせ、しずかは怒って辞退することがある。',\n",
      "               'title': '骨川スネ夫'},\n",
      "              {'passage_id': 639373,\n",
      "               'text': '元の世界でジャイアンやスネ夫にいつもいじめられているのび太の為に、ドラえもんはこの道具を使ってあべこべ世界ののび太に助けを請うが、元の世界に向かったあべこべののび太は、完全に度を越すまでにジャイアンやスネ夫を叩きのめしてしまっただけでなく、悪事を働き続けたため、元の世界のドラえもんとのび太(アニメ版第2作第2期ではのび太の代わりにしずか)が仕掛けたトラップによって、最終的にはあべこべ世界へ追い返されている。テレビアニメ第2作第1期ではあべこべ世界の玉子が登場している。あべこべ世界の玉子は「勉強しなくていいから遊んでらっしゃい」と優しく言っているが、(あべこべ世界の)凶暴なのび太を野放しにしており、教育に問題があった。その為、元の世界の玉子は「のび太に対する教育は厳しいが、のび太を大切に思っている」という事がわかる。',\n",
      "               'title': 'ドラえもんのひみつ道具 (あな-あん)'},\n",
      "              {'passage_id': 724306,\n",
      "               'text': 'まあまあ棒(まあまあぼう)は、「まあまあ棒」(てんとう虫コミックス第23巻に収録)に登場する。先端にX形の付いた棒。(ロボット含む)人がどんなに怒っていても、X形の部分で口をおさえて「まあまあ」となだめれば怒りを静められるという道具。ただし、まあまあ棒は怒りを腹の中へためこんで我慢させるだけであって怒りを消すというわけではないため、あまり我慢をさせすぎると怒りが満タンになって爆発が起こる。作中では、スネ夫がその危険性を知らずにジャイアンをわざと怒らせての使用を繰り返した末に爆発した。ドラえもん曰く、爆発のエネルギーは火山の噴火にも匹敵するそうだが、幸いにも作中では爆発したジャイアンとそばにいたスネ夫が黒こげになるだけで済んだ(それでもちょっとしたガス爆発くらいはあり、ドラえもんの言う様に空地に誘導していなければ人家に被害が出るところだった)。',\n",
      "               'title': 'ドラえもんのひみつ道具 (ま)'},\n",
      "              {'passage_id': 874517,\n",
      "               'text': '動物指キャップ(どうぶつゆびキャップ)は、「動物指キャップ」(てんとう虫コミックススペシャル『ドラえもん '\n",
      "                       'カラー作品集』3巻に収録)に登場する。動物の体の一部を模した指キャップ。これを指先にはめると動物の力が発揮できる。以下の5種類があり、それぞれ能力が異なる。ちなみにドラえもん自身は、手に指がないので使用できない。テレビアニメ第2作第1期「どうぶつ指キャップ」(1989年3月10日放映)では、スネ夫が空き地で父と一緒にプロレスを見に行った話をジャイアンやのび太にした時と、スネ夫がわざとからかいながら実況しジャイアンがプロレスの技を使ってのび太をいじめたところの場面にかつてテレビ朝日のスポーツ中継で使われたスポーツ行進曲でもある神津善行作曲の朝日に栄光あれがBGMとして使われた。',\n",
      "               'title': 'ドラえもんのひみつ道具 (とあ-とこ)'},\n",
      "              {'passage_id': 2713511,\n",
      "               'text': 'ドラえもん、のび太、しずかの3人は動物の星「アニマル星」を訪れる。そこで子犬のチッポやロミと友達になるが、ドラえもんらが地球に帰った後、アニマル星は「ニムゲ」と呼ばれる悪者に襲われる。チッポからのSOSを受けたのび太は、ドラえもんとしずかに、スネ夫、ジャイアンも加え、チッポやロミを助けるためアニマル星へ出発する。',\n",
      "               'title': 'ドラえもん2 アニマル惑星伝説'},\n",
      "              {'passage_id': 1070559,\n",
      "               'text': 'ひい木(ひいき)は、「ひい木」(てんとう虫コミックス第18巻に収録)に登場する。木の形をしたバッジ。これを身につけた者は、誰にでも極端にえこひいきにされるようになる。例えば、玉子がのび太とドラえもんにおやつを持ってきても着用者ののび太だけにあげようとしたり、着用したジャイアンがいたずらをしても彼ではなく近くにいたスネ夫が非難されたりする。 '\n",
      "                       'テレビアニメ第2作第2期では、えこひいきにされる効果はあくまでえこひいきにする側である、周囲の人間の価値観に依存して発揮されるため、それが必ずしも使用者にとって得をする事となるとは限らないという欠点が描写された。作中ではのび太とジャイアンが外面の良いことばかりをして、大人にえこひいきにされているスネ夫を懲らしめるために使った。',\n",
      "               'title': 'ドラえもんのひみつ道具 (ひ)'},\n",
      "              {'passage_id': 718217,\n",
      "               'text': 'なお、薬を飲んだスネ夫が女性2人に本音を言う展開が描かれており、公衆電話で長電話している女性に「無駄な長話してないで電話代われ」、母に「いつも気が利かない」「キィキィわめいてばかりで子供の気持ちを理解しないクソババア」と暴言を吐き、2人を怒らせてしまった。テレビアニメ第2作第2期では、母は一度は認めるとフェイクを出しリサイタルをやることを叱っている。夕方、顔に痣を負い自転車に乗ったジャイアンが直接コンサートの中止を告げに来たが、ドラえもん達はこの時ジャストホンネを飲んでいたことで大喜びしてしまったため、ドラえもん、のび太、スネ夫ら(ほかに安雄とはる夫もいた)はもちろん、しずかまでも激怒したジャイアンに追い掛け回される羽目になった。',\n",
      "               'title': 'ドラえもんのひみつ道具 (しは-しん)'},\n",
      "              {'passage_id': 104512,\n",
      "               'text': 'しずか、ジャイアン、スネ夫の三人とは幼少期からの幼馴染みであり、出木杉も含めて成人後も親交がある。ジャイアンとスネ夫からはいじめや嫌がらせを頻繁に受けている。その為、しずかの家に行こうとしているところをジャイアン一人に邪魔をされたり、ジャイアンとスネ夫の二人に邪魔をされる事がある。のび太自身も仕返しをしようと企んだことがあるが、ことごとくしっぺ返しを食らっている。しかし、ジャイアンとスネ夫はのび太が先生に生徒として完全に否定された際、同情したり、のび太が見えない所で助けようとするなど憎からずも心のどこで友達だと思っている節があり、真の友とも言える描写もある。',\n",
      "               'title': '野比のび太'},\n",
      "              {'passage_id': 174153,\n",
      "               'text': '実際、スネ夫自身もむしろのび太同様ジャイアンにいじめられるほうが多い(これは他の男子勢とも共通する)。そのためチャンスがあれば思いっきりジャイアンを殴りたいと思っている。のび太がドラえもんのひみつ道具でジャイアンに仕返しやイタズラなどをしようとした場合は、のび太やドラえもんと結託することも少なくない(原作連載初期のころは逆にジャイアンを従えて歩く描写が見られた)。また、もしもボックスの効力でのび太がアメリカへ引越しをすることになった際には、それまでの嫌がらせに対して涙を流して謝罪したりする描写から、のび太に嫌がらせをしていることに対して多少なりとも後ろめたさや罪悪感を覚えている節もある。',\n",
      "               'title': '骨川スネ夫'},\n",
      "              {'passage_id': 1083336,\n",
      "               'text': 'そのとき君(のび太)に必要な物が出てくる」と言って残していってくれた「この箱自体が道具を出現させる道具」で、それを使用した結果、ウソ800が出てくることとなった。この薬を飲んだのび太はエイプリルフールにかこつけて「ドラえもんを見かけた」という嘘を付いたジャイアンとスネ夫に仕返しを行い、雨でずぶ濡れにさせた後、スネ夫に対しては「君は犬にかまれない」と言って犬に追わせ、ジャイアンには「母親に褒められる」と言って母親に叱らせた。その直後、帰宅したのび太に玉子が「ドラちゃんはいたの?」と問いかけ、のび太が「ドラえもんはもう帰って来ない」と寂しさまぎれに本音の独り言をこぼしたことで、二度と会えないといって未来へ帰ったドラえもんの話は嘘になり、ドラえもんは再びのび太の元に来ることとなった。川崎市、藤子・F・不二雄ミュージアム内のカフェではこの道具をモチーフにしたハーブティーを飲むことができる。',\n",
      "               'title': 'ドラえもんのひみつ道具 (うあ-うと)'},\n",
      "              {'passage_id': 2268191,\n",
      "               'text': '物語の終盤に入るまでは、主人公であるドラえもん以外のキャラクターでは行動できない。5人にはそれぞれ特長がある。ドラえもんはひみつ道具の攻撃力が高く、のび太は武器の連射が得意、しずかはジャンプが高く、ジャイアンは踏みつけ攻撃の威力が高く、また岩を早く押すことができ、スネ夫は移動スピードが速い。',\n",
      "               'title': 'ドラえもん のび太と妖精の国'},\n",
      "              {'passage_id': 2716911,\n",
      "               'text': '夏休み最後の日、のび太は皆で遊びに行く話が持ち上がったが仲間はずれにされ、憤慨していた。のび太を宥めるため、ドラえもんはテーマパークのチケットを出す。それはドラえもんが抽選で当てた、未来に出来た新しいテーマパークのプレオープンチケットだった。興味を持ったのび太は、しずかを誘いに行くが、いつもの通り、ジャイアン、スネ夫もやってくる。「どら焼き100個をあげる」というスネ夫の甘言に乗せられチケットを渡すドラえもんに対して、のび太は立腹するが、しずかの説得で機嫌を直し、出発を決意。皆の掛け声とともに、テーマパーク、ミニドランドでの夏休み最後の一日が始まる。',\n",
      "               'title': 'ドラえもん みんなで遊ぼう!ミニドランド'},\n",
      "              {'passage_id': 2712554,\n",
      "               'text': 'ドラえもんはタイムマシンのコンピューターを使って迷路ゲームをしていた。のび太、しずか、ジャイアン、スネ夫も加わって、一緒にゲームを楽しんでいると、突然タイムマシンから煙が噴き出し、ドラえもんらは煙に包まれる。ドラえもんが気が付くと、そこには不思議な迷路空間が広がっていた。のび太らの姿はなく、ひみつ道具も散逸する。ドラえもんはミチビキエンゼルの助言のもと、のび太らの捜索、迷路空間からの脱出を試みる。',\n",
      "               'title': 'ドラえもん 対決ひみつ道具!!'},\n",
      "              {'passage_id': 4380664,\n",
      "               'text': '「ドラえもん」LINE '\n",
      "                       'LIVEスペシャル』のタイトルで配信が行われた。出演はドラえもん・のび太・しずか・スネ夫・ジャイアン。MCは高橋茂雄(サバンナ)が担当する。LINE '\n",
      "                       'バブル2がドラえもん仕様に変わり、限定クエストが配信された。指定のミッションをクリアすると、本作のコラボLINEスタンプが入手できる。のび太の宇宙英雄記から連続で行われているゲームアプリ「モンスターストライク」とのコラボレーションが今回でも行われた。3月6日より期間限定でコラボイベントを開催している。映画にちなんだゲーム画面や、ドラえもんやのび太をはじめとするキャラクターが登場する。',\n",
      "               'title': '映画ドラえもん のび太の宝島'},\n",
      "              {'passage_id': 4712825,\n",
      "               'text': '「アニメLIVEチャンネル」では8月7日配信。『声優と夜あそび』コラボ番組。出演は関智一、木村昴、弘中綾香。出演者3人が映画の見どころを紹介する「『映画ドラえもん '\n",
      "                       'のび太の新恐竜』大紹介!」、ジャイアン・スネ夫が『ジャイアンにボエボエ』(キャラクターソング)と映画主題歌『Birthday』を歌う「ジャイアンリサイタル '\n",
      "                       'with '\n",
      "                       'スネ夫」、テレビシリーズに登場するオリジナルのひみつ道具を考える「ガチ採用を目指せ!俺のひみつ道具!」の3コーナーを行った。Abemaの配信では限定特別編として、弘中をゲストに通常の夜あそび企画を行う第2部が公開された。ドラえもんファンの鬼龍院翔をゲストに『ドラえもん』特集を第106回(8月9日)、第107回(8月16日)に分けて2週連続で無料配信した。2020年7月14日より館内「ガイダンスルーム」にて開始。',\n",
      "               'title': '映画ドラえもん のび太の新恐竜'},\n",
      "              {'passage_id': 558289,\n",
      "               'text': '『大長編ドラえもん』は映画化を前提に描かれており、1話完結が原則となる短編に比べて物語の規模が大きいのが特徴。短編では「日常の中の非日常」を軸として狭い町内を舞台に数人の登場人物だけで物語が進むことが多いが、このシリーズでは地球上の未知の領域や、他の惑星、過去の世界といった非日常の世界が舞台に据えられており、その世界の住人や強力な敵などさまざまな人物が登場する。このシリーズでは主要メンバーがドラえもん、のび太、しずか、ジャイアン、スネ夫の5人に固定されており、その作品のゲストキャラとの協力で危機を解決することが多い。主人公であるドラえもんについては、のび太などの活躍によって出番がやや喰われ気味であるが、頭脳でリーダーシップをとる指揮官としてのポジションは確保している。それだけでなく、作品によっては主人公に相応しい活躍をしたり、終盤で重要な役割を担うこともある。',\n",
      "               'title': '大長編ドラえもん'},\n",
      "              {'passage_id': 1070652,\n",
      "               'text': '人間すごろく(にんげんすごろく)は、「人間すごろく」(てんとう虫コミックス第45巻に収録)に登場する。ドラえもんが自分で作ったすごろく。振り出しからあがりに至るまでのマスが実際に周囲の町の道のりになっており、コマは実在の人物を模している。コマを動かすとその人物もその通りに動き、「歌を歌う」「3回回ってワンと言う」などの罰ゲームは、すごろくで遊んでいるプレイヤーではなく、コマの人物がやることになる。作中では、ふりだしはのび太の町の空き地、あがりは野比家、コマはジャイアンとスネ夫に割り振られていた。',\n",
      "               'title': 'ドラえもんのひみつ道具 (に)'},\n",
      "              {'passage_id': 1428224,\n",
      "               'text': 'その頃、元の世界でも大質量天体の地球衝突回避が確認されていた。ドラえもんとのび太は一連の大冒険が夢のようだと思いながらも、しずか、スネ夫、ジャイアンの下へ駆け出していった。ただし空き地の木には乗り捨てられる結果になった箒が差さっていた。',\n",
      "               'title': '映画ドラえもん のび太の新魔界大冒険 〜7人の魔法使い〜'},\n",
      "              {'passage_id': 1097104,\n",
      "               'text': '※苗字の〔〕は、本人または親の結婚によって姓が変わった登場人物の結婚後の苗字を表す。',\n",
      "               'title': '翔んだカップル'},\n",
      "              {'passage_id': 2710610,\n",
      "               'text': 'スネ夫は自分が所持する大きなおもちゃのロボットの自慢話をしていた。それに対しのび太は「もっと大きいおもちゃを持っている」とむきになってでたらめを語ってしまう。スネ夫やジャイアンに「どうせドラえもんに頼んで出してもらうんだろう」と問いただされたのび太は「取りに行ってくる」と言いつけ帰宅しドラえもんに大きなおもちゃを出してもらおうとする。しかしドラえもんは留守だった。と、その時突然小人風のしゃべる不思議なおもちゃが現れ、「トイズランド」と呼ばれる島へ招待される。そこには大きなロボットもいると聞いたのび太は大喜び。小人風のおもちゃはのび太にトイズランドへ向かうために必要な笛を渡す。てっきりドラえもんがひそかに用意していたものとのび太は思い込んでいたが、帰って来たドラえもんはのび太の持つ笛に目を白黒させ、逆にのび太もトイズランドがドラえもんの仕業ではなかったことに驚く。',\n",
      "               'title': 'ドラえもん2 のび太のトイズランド大冒険'},\n",
      "              {'passage_id': 879633,\n",
      "               'text': 'ジャイアンとスネ夫の言動や、ドラえもんのそれを予期した発言から、装着中は雪は増やす一方で、ダイヤルを逆に回しても一度増やした雪を減らす事はできないと思われる。そのため雪を増やしすぎて転落死するほどの高度に来てしまうと、地上に帰れなくなってしまう虞がある。',\n",
      "               'title': 'ドラえもんのひみつ道具 (いあ-いと)'},\n",
      "              {'passage_id': 962947,\n",
      "               'text': 'むしめがねは、「むしめがねでへんしん」(藤子・F・不二雄大全集第5巻に収録)に登場する。この虫眼鏡を覗くと、体が昆虫に変身する。反対側から覗くと元の姿に戻る。これによりのび太はトンボ、ドラえもんはテントウムシ、しずかはチョウ、スネ夫はハチに変身したが、どのように虫の種類を指定するかは作中では描かれていない。ところが、ジャイアンは「ちゃわんむし」に変身してしまった。',\n",
      "               'title': 'ドラえもんのひみつ道具 (む)'},\n",
      "              {'passage_id': 266903,\n",
      "               'text': '息をするようにお芝居されている。あの域に行きたい。」と述べている '\n",
      "                       '。元々スネ夫のファンだった関だが、リニューアル版『ドラえもん』のオーディションは、当初スネ夫役ではなくドラえもん役で参加オファーが届いていた。関自身も「さすがにドラえもん役は無理だろう」と思いつつ一応オーディションに臨んだが、落選。後日行われたスネ夫のオーディションに参加し、内定した。またスネ夫の声以外にも次回予告やスペシャル、ドラえもん映画の予告編などにおけるナレーション、加えてミニコーナー「スネ夫としげお」も担当している。関はリニューアル版以前にもドラえもん映画作品でスネ夫に関係のあるいくつかのキャラクターの声を演じている。',\n",
      "               'title': '関智一'},\n",
      "              {'passage_id': 542649,\n",
      "               'text': 'ドラえもん曰く、未来世界においてはこの機械で確かにコピー人間を作ることができるが、同じ人間が2人もいれば何かとトラブルの元になる上、コピーを作った者は子を持つ親同様に教育や育成の義務が生じるため、コピー人間を作る者は滅多にいないという。そのためか「とりけしスイッチ」が設けられており、これを押すとコピー人間は、作成元の髪の毛などの状態にまで戻る。作中ではのび太がジャイアンとスネ夫のクローンを作り出した。育成や教育の必要があるのにも関わらず、計算問題を即答えたり、跳び箱を一回で成功しており、クローンの性格がジャイアンとスネ夫と同じ意地悪で乱暴な性格になっていた。',\n",
      "               'title': 'ドラえもんのひみつ道具 (くな-くん)'},\n",
      "              {'passage_id': 912597,\n",
      "               'text': '基本的にのび太以外だとジャイアンやスネ夫にひみつ道具を貸そうとすることは少ないが、妙に大らかでお人好しなところもあり、のび太が止めてもスネ夫やジャイアンにひみつ道具を貸して、彼らがトラブルを起こしてしまうこともある。のび太が全治1ヶ月の大怪我をするという予言は外れたものの、結局怪我をしてしまったのび太の気持ちを考えずに喜ぶという無神経な対応をしたこともある。あらゆるひみつ道具を持っているドラえもんだが、道具の管理はずさんであり、危険な道具を部屋に置きっぱなしにして出かけたり、回によっては部屋でとても危険な道具を見つけたのび太に「ぜったいに、いじるなよ!」と言い、道具をしまうでもなく出かけてしまったこともあるなどの行動が目立つ。',\n",
      "               'title': 'ドラえもん (キャラクター)'},\n",
      "              {'passage_id': 3894564,\n",
      "               'text': '子どもたちに大人気のヒーロー番組『ミラクルヒーロー銀河防衛隊』を見て「自分もヒーローになれたら」と憧れるのび太。一方、同じくその番組を見たスネ夫とジャイアンがしずかを誘ってヒーローものの映画撮影を始めた。それを目の当たりにしたのび太も「参加したい」と懇願するがヒーローの相手の怪獣役を与えられてしまい、ジャイアンに投げ飛ばされてしまう。家に帰ってきたのび太からその経緯を聞き、自身も番組のファンだと自称するドラえもんはのび太と再びジャイアンたちの元へ向かい、監督ロボット・バーガー監督の映画制作の能力と衣装のスーツを「グレードアップライト」による力で超能力を使えるようにして本格的な映画を撮ることになった。撮影後、そこへ現れた謎の人物・アロンから「故郷のポックル星を救って欲しい」と頼まれ、アロンの乗ってきた宇宙船に乗り込みポックル星へと向かう。',\n",
      "               'title': '映画ドラえもん のび太の宇宙英雄記'},\n",
      "              {'passage_id': 1792399,\n",
      "               'text': '探し物は何ですカ(さがしものはなんですカ)は、映画『がんばれ!ジャイアン!!』に登場する、ドラえもんの道具。探し物や探し人を探し出す道具。漫画と映画では外観、使用方法が大きく異なる。',\n",
      "               'title': '映画ドラえもんのひみつ道具'},\n",
      "              {'passage_id': 542619,\n",
      "               'text': '組み立て円盤セット(くみたてえんばんセット)は、「ニセ宇宙人」(てんとう虫コミックス10巻に収録)に登場する。未来の世界の幼稚園児たちの遊び道具で、プラモデルのような組み立て式の空飛ぶ円盤。人が乗れるほどのサイズで、実際に乗って操縦し、空を飛ぶことができる。 '\n",
      "                       'のび太とドラえもんがスネ夫とジャイアンにラジコン宇宙人と合わせて使用し、「日本の首相に会いたい」と言う要求にだまされたスネ夫が慌てて鈴木首相(初期の版では三木首相)に電話をかけていた(何故かスネ夫は電話番号は知っていたが、総理大臣が総理大臣公邸に住んでいることは知らなかった。ちなみに三木首相は公邸に住み、鈴木首相は私邸に住んでいた。)。テレビアニメ第2作第2期ではスネ夫が「総理大臣は国会議事堂にいるかもしれない」と考え、国会に電話を掛けたり、国会に行って守衛に総理大臣の住所を聞き出そうとした。',\n",
      "               'title': 'ドラえもんのひみつ道具 (くな-くん)'},\n",
      "              {'passage_id': 1764456,\n",
      "               'text': 'スネ夫からは「これ以上下手な歌を聴きたくなかったから」という理由で絵を描くことを趣味にするようおだてて勧めたが、当のスネ夫はジャイアンの絵を「むちゃくちゃ」と表していた。ただ作中後半になると「いい色出しているね」と評論家から褒められたり、絵が得意なスネ夫の書いた絵に劣らぬキャラクター原案を描いたり、意外と実力を秘めている。好物は豆、たい焼き、饅頭、ラーメン、冷やし中華などで、大長編ではラーメンやカツ丼を食べているシーンが多い。のび太やスネ夫が好き嫌いが多いのに対して基本的に食べ物の好き嫌いは無く、何でも美味しく食べている。テレビアニメ第2作第1期ではピーマンが好物という設定。尊敬する人物は宮本武蔵と柔道十段のおじさん(後述)。',\n",
      "               'title': '剛田武'},\n",
      "              {'passage_id': 104522,\n",
      "               'text': '絵が大の得意な父とは正反対で大の苦手であり、そのためジャイアンとスネ夫に馬鹿にされることがある。また、美術評論家であるしずかのおじに自分の絵を評価してもらうが、幼稚園の頃描いたものだと勘違いされ、ドラえもんには犬の絵を猫の絵と勘違いされたり、幼稚園のころ描いた絵を「今とあまり変わらない」と言われたりする。『のび太の創世日記』では、ドラえもんが恐竜の絵をトカゲの絵だと勘違いしている。自分でも絵の下手さは自覚しており、しずかをモデルに人物画を描いた際には誤魔化してしずかに見せず、後で自分の絵を「こんなの見せたらどんなに怒るか」と評している。ただし、絵を描くこと自体は決して嫌いではないようで、上手い下手を気にせずに、一人で漫画や落書きを楽しそうに描いていることもある。',\n",
      "               'title': '野比のび太'},\n",
      "              {'passage_id': 888822,\n",
      "               'text': '今作では『ドラえもん '\n",
      "                       'のび太の宇宙小戦争』(1985年)以来11年ぶりにオープニングアニメーションでのび太、しずか、ジャイアン、スネ夫が登場した。2004年には同作を4週にわたりNG集を含めた「完全版」(番組内では「特別版」)をテレビスペシャルとして放送した。ただし一部の本編やエンドロールがカットされているため、一般的に「完全版」と呼べるものとは異なる。NG集はブリッジアニメとして放送。ドラえもんたちがNGを出しているものであり(ドラえもんが列車に乗る直前に切符でなく切手を出す、宇宙の場面では宇宙忍者でなくのび太のママが登場してしまう、など)、NGのために新規の作画が描き下ろされた。その後柳沢慎吾などとドラえもんがアニメで共演している。',\n",
      "               'title': 'ドラえもん のび太と銀河超特急'},\n",
      "              {'passage_id': 879608,\n",
      "               'text': '例としてのび太はパワーを最大にした結果、ジャイアンとスネ夫を同時に相手して倒すほど体力が増大したが、ルックスが低下しすぎてドラえもんでものび太と認識できないほど顔が崩れ、IQが低下しすぎて自分が誰なのかも分からなくなってしまった。テレビアニメ第2作第2期では、のび太がジャイアンのパワーを最低にしたところ、IQとルックスがその分上がり「きれいなジャイアン」のような美男子となった。こちらのジャイアンは顔は凛々しく、声も変化前と比べて爽やかになっている。しかし、パワーはそれほど変化せず、依然としてのび太より強かった。',\n",
      "               'title': 'ドラえもんのひみつ道具 (いあ-いと)'},\n",
      "              {'passage_id': 679524,\n",
      "               'text': '本作は『西遊記』がモチーフであり、唐の時代の中国を舞台としている。ストーリーは妖怪に支配されるパラレルワールドと化した世界を修正するため、のび太たちが『西遊記』の登場人物に扮して、ドラえもんと共に妖怪たちと戦うというものである。今作はドラえもんのひみつ道具を発端として発生した事件の影響が世界規模で現実世界に悪影響を及ぼしてしまうという展開となっており、「日常の中の非日常」「どんな歴史を揺るがす大事件であってもなるべく仲間内で解決し、一般社会には影響を残さない」というシリーズのテーマを覆した初の作品であり、過失とはいえドラえもん自身が敵役を生み出してしまった点で、他の作品と一線を画している。この戦いの際、のび太が孫悟空、ジャイアンは猪八戒、スネ夫は沙悟浄、しずかは三蔵法師の役に就いた。',\n",
      "               'title': 'ドラえもん のび太のパラレル西遊記'},\n",
      "              {'passage_id': 1764450,\n",
      "               'text': '子守りロボットのドラえもんよりも子守りも上手く、面倒見のいい所もある。スネ夫からラジコンを奪い、ラジコン機が民家の敷地などあらぬ方向に行ってしまうと、「おれ、知〜らない」といってプロポだけをスネ夫に突き返して逃げたり、ジャイアン自身の問題であるにもかかわらず、嫌な事をのび太に押し付けるなど、無責任な面もある。のび太がしずかの家に行こうとしているのを単独で邪魔したり、スネ夫と共に邪魔することがしばしばある。スネ夫と共におつかいをしている途中ののび太にいたずらを仕掛け、のび太がママに叱られる原因を作ることもある。両親には全く頭が上がらず、これらの行為をしている所を見つかって母ちゃんからお仕置きを受け、強制的に退場させられるパターンが多い。逆に叱らなくなると、それをいいことにして乱暴をさらにエスカレートさせていた。',\n",
      "               'title': '剛田武'},\n",
      "              {'passage_id': 796501,\n",
      "               'text': '1990年、ドイツ・ライプツィヒにて、ドイツ人でオペラ歌手の父と日本人でバロック音楽のソリストで声楽家の母との間に生まれる。7歳までをドイツで過ごし、日本へ移住した後に劇団日本児童へ所属。2002年ミュージカル・『アニー』でタップダンサーとして出演し、キャリアをスタートさせた。小学生のころに、ものまね番組『まねキン』(日本テレビ)に羽賀研二に似ている小学生として出演。そのことがきっかけで『ものまねバトル』(日本テレビ)で「ミニミニ羽賀研二」という名前で何度か出演した。2005年4月15日よりテレビ朝日系アニメ『ドラえもん』にて、たてかべ和也に代わりジャイアンこと剛田武の声を担当。「大人になってから周囲に自慢しよう」と思っていたが、3月にジャイアン役が公になってからは同級生や先輩後輩から「ジャイアン(先輩)」と呼ばれるようになり、「剛田武(=ジャイアン)として生きていく」ことを覚悟する。',\n",
      "               'title': '木村昴'},\n",
      "              {'passage_id': 912594,\n",
      "               'text': '」と激昂したりもすれば、マンガを読んだりドラ焼きを食べたりしながら冷たい対応をするなど、その場のテンションでのび太に対する対応がかなり異なる。初期の頃は前述の通りのび太の話を聞くなり部屋を飛び出して復讐するなどかなりエキセントリックであったが、後期になるにつれて「いつものことじゃない」「しょっちゅうだもの。いちいち気にしてらんないよ!」などと言い放つなど冷静になっていく。ただし、のび太の無気力さや無頓着さ、不甲斐なさに耐えかねて、のび太が道具をせがまなくてもドラえもんが自ら道具を出すこともある。また、ジャイアンにやられた時とスネ夫にやられた時の対応が多少異なっている(ジャイアンの時は「やられたらやりかえせ!」などと叱咤し、すぐには道具を出したりしないが、スネ夫の時は「負けてたまるか!」などと言ってすぐに道具を出す場合が多い)。',\n",
      "               'title': 'ドラえもん (キャラクター)'}],\n",
      " 'positive_passage_indices': [11, 12, 16, 20, 32, 35],\n",
      " 'qid': 'ABC01-01-0002',\n",
      " 'question': '人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう?',\n",
      " 'section': 'ペーパー筆記',\n",
      " 'timestamp': '2003/03/30'}\n"
     ]
    }
   ],
   "source": [
    "pprint(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbf23002-45a3-4d74-9e7e-becc338f798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正例と負例のパッセージを持たない事例を除去する\n",
    "train_dataset = train_dataset.filter(lambda x: len(x['positive_passage_indices']) > 0 and len(x['negative_passage_indices']) > 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10028c2b-b587-4b6b-9f19-d747e184d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データセットの各事例の正例パッセージの先頭だけ残す（質問と最も関連度が高いため）\n",
    "\n",
    "def filter_passages(example: dict) -> dict:\n",
    "    \"\"\"訓練セットの各事例で、正例のパッセージを最初の一つのみ残す\"\"\"\n",
    "    example[\"positive_passage_indices\"] = [example[\"positive_passage_indices\"][0]]\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(filter_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9aad4144-1720-4371-8742-239369e5157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['qid', 'competition', 'timestamp', 'section', 'number', 'original_question', 'original_answer', 'original_additional_info', 'question', 'answers', 'passages', 'positive_passage_indices', 'negative_passage_indices'],\n",
      "    num_rows: 19596\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# datasetの確認\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9d38474-6bcc-4730-bf37-0810f8a734a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1249466e2244ad9aa7afa0abe22162e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 検証データセットにも同様の処理を行う\n",
    "\n",
    "valid_dataset = load_dataset(\"llm-book/aio-retriever\", split=\"validation\")\n",
    "valid_dataset = valid_dataset.filter(lambda x: len(x['positive_passage_indices']) > 0 and len(x['negative_passage_indices']) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9130440c-e965-4036-a1de-8154f65369f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b903c5e96f524d0baa326db5960f20e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/864 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 検証データでは正例パッセージと負例パッセージの先頭の事例のみを使用するようにする。\n",
    "# ランダムで選ばれたものを使うと再現性がとれなくなるため？？\n",
    "\n",
    "def filter_passages(example: dict) -> dict:\n",
    "    \"\"\"検証セットの各事例で、正例のパッセージと負例パッセージを最初の一つのみ残す\"\"\"\n",
    "    example[\"positive_passage_indices\"] = [example[\"positive_passage_indices\"][0]]\n",
    "    example[\"negative_passage_indices\"] = [example[\"negative_passage_indices\"][0]]\n",
    "\n",
    "    return example\n",
    "\n",
    "valid_dataset = valid_dataset.map(filter_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe007151-efa8-45e0-b23b-f9bfa3273ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['qid', 'competition', 'timestamp', 'section', 'number', 'original_question', 'original_answer', 'original_additional_info', 'question', 'answers', 'passages', 'positive_passage_indices', 'negative_passage_indices'],\n",
      "    num_rows: 864\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e897f-82e6-41e2-b48d-9eb6049dc161",
   "metadata": {},
   "source": [
    "### トークナイザとcollate関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "830a4543-39c5-4c9e-a1ed-cdfa714855ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/vocab.txt\n",
      "loading file spiece.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n"
     ]
    }
   ],
   "source": [
    "# tokenizer\n",
    "base_model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "32b266f1-c70a-49e2-ae52-935c940af7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate\n",
    "def collate_fn(examples: list[dict]) -> dict[str, BatchEncoding | Tensor]:\n",
    "    \"\"\"BPRの訓練・検証データのミニバッチを作成\"\"\"\n",
    "    questions: list[str] = []\n",
    "    passage_titles: list[str] = []\n",
    "    passage_texts: list[str] = []\n",
    "\n",
    "    for example in examples:\n",
    "        questions.append(example['question'])\n",
    "\n",
    "        # 事例の正例と負例を一つランダムで取得する\n",
    "        positive_passage_idx = random.choice(example['positive_passage_indices'])\n",
    "        negative_passage_idx = random.choice(example['negative_passage_indices'])\n",
    "\n",
    "\n",
    "        # ピックアップされたidxのtitleとtextを格納\n",
    "        passage_titles.extend(\n",
    "            [\n",
    "            example[\"passages\"][positive_passage_idx]['title'],\n",
    "            example[\"passages\"][negative_passage_idx]['title']\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        passage_texts.extend([\n",
    "            example[\"passages\"][positive_passage_idx]['text'],\n",
    "            example[\"passages\"][negative_passage_idx]['text'],\n",
    "        ])\n",
    "        \n",
    "\n",
    "    # 質問とパッセージをtokenize\n",
    "    tokenized_question = tokenizer(\n",
    "        questions,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    tokenized_passage = tokenizer(\n",
    "        passage_titles,\n",
    "        passage_texts,\n",
    "        padding=True,\n",
    "        truncation=\"only_second\", # 2番目のペア（passage_texts）のみtrancateされる\n",
    "        max_length=128,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "\n",
    "     # 質問とパッセージのスコア行列における正例の位置を示すTensorを作成する\n",
    "    # 行列の [0, 1, 2, ..., len(questions) - 1] 行目の事例（質問）に対して\n",
    "    # [0, 2, 4, ..., 2 * (len(questions) - 1)] 列目の要素（パッセージ）が\n",
    "    # 正例となる\n",
    "    labels = torch.arange(0, 2 * len(questions), 2)\n",
    "    return {\n",
    "        \"tokenized_questions\": tokenized_question,\n",
    "        \"tokenized_passages\": tokenized_passage,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bccedd-c4c1-4000-853d-b9a9765c0204",
   "metadata": {},
   "source": [
    "### モデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7997adf6-f119-4f12-ab77-1924f787fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRModel(nn.Module):\n",
    "    \"\"\"BPRモデル\"\"\"\n",
    "    def __init__(self, base_model_name: str):\n",
    "        super().__init__()\n",
    "\n",
    "        # エンコーダーの構築\n",
    "        self.question_encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        self.passage_encoder = AutoModel.from_pretrained(base_model_name)\n",
    "\n",
    "        # 訓練時のステップ数（バイナリの損失計算に使用）\n",
    "        self.global_step = 0\n",
    "\n",
    "\n",
    "    def binary_encode(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"実数埋め込みをバイナリ変換\"\"\"\n",
    "        if self.training:\n",
    "            # 訓練時\n",
    "            return torch.tanh(x * math.pow((1.0 + self.global_step * 0.1), 0.5))\n",
    "        else:\n",
    "            # 推論時\n",
    "            return torch.where(x >= 0, 1.0, -1.0).to(x.device)\n",
    "\n",
    "\n",
    "    def encode_questions(self, tokenized_questions: BatchEncoding) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"質問を実数埋め込みとバイナリに変換\"\"\"\n",
    "        encoded_questions = self.question_encoder(**tokenized_questions).last_hidden_state[:, 0]\n",
    "        binary_encoded_questions = self.binary_encode(encoded_questions)\n",
    "\n",
    "        return encoded_questions, binary_encoded_questions\n",
    "    \n",
    "    def encode_passages(self, tokenized_passages: BatchEncoding) -> Tensor:\n",
    "        \"\"\"パッセージをバイナリに変換\"\"\"\n",
    "        encoded_passages = self.passage_encoder(**tokenized_passages).last_hidden_state[:, 0]\n",
    "        binary_encoded_passages = self.binary_encode(encoded_passages)\n",
    "        \n",
    "        return binary_encoded_passages\n",
    "\n",
    "\n",
    "    def compute_loss(self, \n",
    "                     encoded_questions: Tensor, \n",
    "                     binary_encoded_questions: Tensor, \n",
    "                     binary_encoded_passages: Tensor, \n",
    "                     labels: Tensor,\n",
    "                    )-> Tensor:\n",
    "        \n",
    "        \"\"\"BPRの損失計算\"\"\"\n",
    "        num_questions = encoded_questions.size(0)\n",
    "        num_passages = binary_encoded_passages.size(0) # 正例と負例があるのでquestionより、2倍のバッチ数\n",
    "\n",
    "        # 候補パッセージの計算\n",
    "        # 質問のバイナリとパッセージのバイナリの内積をスコアに用いて\n",
    "        # 正例パッセージのスコアと負例パッセージスコアのランキング損失を計算する。\n",
    "        binary_scores = torch.matmul(binary_encoded_questions, binary_encoded_passages.transpose(0, 1)) # (バッチ数, 次元数)* (次元数, バッチ数(正例＋負例)) => (バッチ数、バッチ数*2)\n",
    "        \n",
    "        positive_binary_mask = F.one_hot(labels, num_classes=num_passages).bool() # \n",
    "        positive_binary_scores = torch.masked_select(binary_scores, positive_binary_mask).repeat_interleave(num_passages - 1) # 各question(行)に対応した正例のスコアがすべての列（num_passages - 1分）に反映される\n",
    "        negative_binary_scores = torch.masked_select(binary_scores, ~positive_binary_mask) # num_passages - 1の列数になることに注意\n",
    "        target =torch.ones_like( positive_binary_scores).long()\n",
    "        loss_cand = F.margin_ranking_loss(positive_binary_scores,\n",
    "                                          negative_binary_scores,\n",
    "                                          target,\n",
    "                                          margin=0.1\n",
    "                                         )\n",
    "\n",
    "        # 候補パッセージのリランキングの損失を計算する\n",
    "        # 質問の実数埋め込みとパッセージのバイナリ埋め込みの内積を\n",
    "        # スコアに用いて、正例パッセージのスコアと負例パッセージのスコアの\n",
    "        # 交差エントロピー損失を計算する\n",
    "        dense_scores = torch.matmul(encoded_questions, binary_encoded_passages.transpose(0, 1))\n",
    "        loss_rerank = F.cross_entropy(dense_scores, labels)\n",
    "        loss = loss_cand + loss_rerank\n",
    "        return loss\n",
    "\n",
    "    # 順伝搬\n",
    "    def forward(self, tokenized_questions: BatchEncoding, tokenized_passages: BatchEncoding, labels: Tensor) -> ModelOutput:\n",
    "        # questionとpassageを埋め込み\n",
    "        encoded_questions, binary_encoded_questions = (self.encode_questions(tokenized_questions))\n",
    "        binary_encoded_passages = (self.encode_passages(tokenized_passages))\n",
    "\n",
    "        \n",
    "        # BPRの計算\n",
    "        loss = self.compute_loss(encoded_questions, binary_encoded_questions, binary_encoded_passages, labels)\n",
    "\n",
    "        if self.training:\n",
    "            self.global_step += 1\n",
    "\n",
    "        return ModelOutput(loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3c93675b-9846-4d52-adbe-20ba2f79556a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-v3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/pytorch_model.bin\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v3 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-v3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cl-tohoku--bert-base-japanese-v3/snapshots/65243d6e5629b969c77309f217bd7b1a79d43c7e/pytorch_model.bin\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v3 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# modelの初期化\n",
    "model = BPRModel(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0a9e9ab2-f54e-401a-9846-6269c4decee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "# BPRの訓練のハイパーパラメータを設定する\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs_bpr\",  # 結果の保存先フォルダ\n",
    "    per_device_train_batch_size=32,  # 訓練時のバッチサイズ\n",
    "    per_device_eval_batch_size=32,  # 評価時のバッチサイズ \n",
    "    learning_rate=1e-5,  # 学習率\n",
    "    max_grad_norm=2.0,  # 勾配クリッピングにおけるノルムの最大値 ===========重要な追加点==============\n",
    "    num_train_epochs=20,  # 訓練エポック数\n",
    "    warmup_ratio=0.1,  # 学習率のウォームアップを行う長さ\n",
    "    lr_scheduler_type=\"linear\",  # 学習率のスケジューラの種類\n",
    "    evaluation_strategy=\"epoch\",  # 検証セットによる評価のタイミング\n",
    "    logging_strategy=\"epoch\",  # ロギングのタイミング\n",
    "    save_strategy=\"epoch\",  # チェックポイントの保存のタイミング\n",
    "    save_total_limit=1,  # 保存するチェックポイントの最大数\n",
    "    fp16=True,  # 自動混合精度演算の有効化\n",
    "    remove_unused_columns=False,  # データセットの不要フィールドを削除するか\n",
    "    report_to='all'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a79d8180-ab96-46fb-9f8d-ee04dbffd74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model = model,\n",
    "                  args=training_args,\n",
    "                  data_collator=collate_fn,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=valid_dataset\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "79b3f236-595e-4a43-875a-37dbf9b434e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19,596\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12,260\n",
      "  Number of trainable parameters = 222,414,336\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12260' max='12260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12260/12260 4:35:18, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.434800</td>\n",
       "      <td>1.873021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.716800</td>\n",
       "      <td>1.404888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.506300</td>\n",
       "      <td>1.349858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.366500</td>\n",
       "      <td>1.365851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.296700</td>\n",
       "      <td>1.323755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.248300</td>\n",
       "      <td>1.382280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>1.489815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.176500</td>\n",
       "      <td>1.453172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>1.571055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.151200</td>\n",
       "      <td>1.603697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>1.760706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.119000</td>\n",
       "      <td>1.773908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>1.751889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>1.772775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>1.840315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.088200</td>\n",
       "      <td>1.887938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>1.916091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>1.853354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>1.887567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>1.851667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-613\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-1226\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-613] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-1839\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-1226] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-2452\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-1839] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-3065\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-2452] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-3678\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-3065] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-4291\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-3678] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-4904\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-4291] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-5517\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-4904] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-6130\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-5517] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-6743\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-6130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-7356\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-6743] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-7969\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-7356] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-8582\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-7969] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-9195\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-8582] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-9808\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-9195] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-10421\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-9808] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-11034\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-10421] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-11647\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-11034] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 864\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs_bpr/checkpoint-12260\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [outputs_bpr/checkpoint-11647] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12260, training_loss=0.4101210082335729, metrics={'train_runtime': 16519.3443, 'train_samples_per_second': 23.725, 'train_steps_per_second': 0.742, 'total_flos': 0.0, 'train_loss': 0.4101210082335729, 'epoch': 20.0})"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "696c3380-0a99-4213-aba5-f1693245226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in outputs_bpr/question_encoder/config.json\n",
      "Model weights saved in outputs_bpr/question_encoder/model.safetensors\n",
      "tokenizer config file saved in outputs_bpr/question_encoder/tokenizer_config.json\n",
      "Special tokens file saved in outputs_bpr/question_encoder/special_tokens_map.json\n",
      "Configuration saved in outputs_bpr/passage_encoder/config.json\n",
      "Model weights saved in outputs_bpr/passage_encoder/model.safetensors\n",
      "tokenizer config file saved in outputs_bpr/passage_encoder/tokenizer_config.json\n",
      "Special tokens file saved in outputs_bpr/passage_encoder/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('outputs_bpr/passage_encoder/tokenizer_config.json',\n",
       " 'outputs_bpr/passage_encoder/special_tokens_map.json',\n",
       " 'outputs_bpr/passage_encoder/vocab.txt',\n",
       " 'outputs_bpr/passage_encoder/added_tokens.json')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 質問エンコーダを保存\n",
    "question_encoder_path = \"outputs_bpr/question_encoder\"\n",
    "model.question_encoder.save_pretrained(question_encoder_path)\n",
    "tokenizer.save_pretrained(question_encoder_path)\n",
    "\n",
    "# パッセージエンコーダを保存\n",
    "passage_encoder_path = \"outputs_bpr/passage_encoder\"\n",
    "model.passage_encoder.save_pretrained(passage_encoder_path)\n",
    "tokenizer.save_pretrained(passage_encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5bda9ced-2ad1-4f77-ba14-2e0126b5354f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a= collate_fn(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7f394ada-f158-4ea6-b975-d37b1a14a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_collate_fn(examples: list[dict]) -> dict[str, BatchEncoding | Tensor]:\n",
    "    \"\"\"BPRの訓練・検証データのミニバッチを作成\"\"\"\n",
    "    questions: list[str] = []\n",
    "    passage_titles: list[str] = []\n",
    "    passage_texts: list[str] = []\n",
    "    count = 0\n",
    "\n",
    "    for example in examples:\n",
    "        questions.append(example['question'])\n",
    "\n",
    "        # 事例の正例と負例を一つランダムで取得する\n",
    "        positive_passage_idx = random.choice(example['positive_passage_indices'])\n",
    "        negative_passage_idx = random.choice(example['negative_passage_indices'])\n",
    "\n",
    "\n",
    "        # ピックアップされたidxのtitleとtextを格納\n",
    "        passage_titles.extend(\n",
    "            [\n",
    "            example[\"passages\"][positive_passage_idx]['title'],\n",
    "            example[\"passages\"][negative_passage_idx]['title']\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        passage_texts.extend([\n",
    "            example[\"passages\"][positive_passage_idx]['text'],\n",
    "            example[\"passages\"][negative_passage_idx]['text'],\n",
    "        ])\n",
    "\n",
    "        count += 1\n",
    "        if count >= 2:\n",
    "            break\n",
    "        \n",
    "\n",
    "    # 質問とパッセージをtokenize\n",
    "    tokenized_question = tokenizer(\n",
    "        questions,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    tokenized_passage = tokenizer(\n",
    "        passage_titles,\n",
    "        passage_texts,\n",
    "        padding=True,\n",
    "        truncation=\"only_second\", # 2番目のペア（passage_texts）のみtrancateされる\n",
    "        max_length=128,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "\n",
    "     # 質問とパッセージのスコア行列における正例の位置を示すTensorを作成する\n",
    "    # 行列の [0, 1, 2, ..., len(questions) - 1] 行目の事例（質問）に対して\n",
    "    # [0, 2, 4, ..., 2 * (len(questions) - 1)] 列目の要素（パッセージ）が\n",
    "    # 正例となる\n",
    "    labels = torch.arange(0, 2 * len(questions), 2)\n",
    "    return {\n",
    "        \"tokenized_questions\": tokenized_question,\n",
    "        \"tokenized_passages\": tokenized_passage,\n",
    "        \"labels\": labels,\n",
    "        \"questions\": questions,\n",
    "        \"passage_titles\": passage_titles\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "40297c8c-7b78-499a-aecc-94c563369384",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = test_collate_fn(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5f76abfb-5acb-4358-abc2-477c138aff7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([0, 2]),\n",
      " 'passage_titles': ['ビヨンセ', 'クロエ・ベネット', 'ドラえもん', '剛田武'],\n",
      " 'questions': ['「abc 〜the first〜」へようこそ!さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう?',\n",
      "               '人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう?'],\n",
      " 'tokenized_passages': {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      "                        'input_ids': tensor([[    2,   555, 17705,  7031,     3, 12578,   464, 22529,   464,    69,\n",
      "         13480, 15414,    59, 16493,   430,   384,   555, 17705,  7031,   464,\n",
      "         13932, 26182, 12488, 13268,   460, 23757,   500, 23289,   441,   384,\n",
      "         23050, 12766, 13315,   500, 14930, 13644,   461,   441,   449, 25403,\n",
      "           395, 13450, 18833,   182,    98, 13290,     1,   396,   500, 13163,\n",
      "           441,   456, 12483,   385,     3,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    2, 13101,  7034,   593, 18460, 12507,     3, 13101,  7034,   593,\n",
      "         18460, 12507,    23, 13794, 29944,  7084, 31477, 31926,    27, 13794,\n",
      "         29944,  7084,    70, 16404,    27,  3256,  1205,  4060,    27, 13668,\n",
      "          2002,    35,  2806, 12524,  2698,    28,    24,   465,   384, 12578,\n",
      "         13011,  1432,   464, 13896,   385, 17594, 12620,  7035, 17725,   464,\n",
      "           397, 13450, 22274, 29064,  7222,  7084,   398,   457, 15396,   500,\n",
      "           441,   456,   422,   449,   385, 18833, 14014,   464, 12603, 13046,\n",
      "           397, 29257, 17445,  7147, 24697,   398,   461, 18934, 12503,  2115,\n",
      "           457, 12686,   441,   384, 13586, 18833,   464, 12603, 13046,   397,\n",
      "         20214,   593, 13811,   593, 20078,   398,   461, 14093, 12686,   441,\n",
      "           456, 12483,   385,     3,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    2, 20344,     3, 24396,   430,   428, 17823,   500, 29003,  7274,\n",
      "          7234,   458, 16478,   456, 12483,   458,   384, 16720,   384, 15597,\n",
      "         12488,   458,   484, 12564,  2116,   464, 14871,   500, 23245,  1567,\n",
      "           430, 26269,   384, 13602, 14794,   461, 15185,   456, 13884,   456,\n",
      "           431,   449, 20344,   458,   384, 24396,   464,  1746,   464,  1746,\n",
      "           464,   531,  7141,  7033,   430, 14196,   449,   385,   531,  7141,\n",
      "          7033, 16202,   384, 14167,  7265,  7533,  7203,   464, 24396,   465,\n",
      "         12818,   461,  1004,   449,  2127,   484, 31978,   464,   610,  8043,\n",
      "           461, 22806,   494,   384, 12587,   464, 21540,   430, 13900,   457,\n",
      "         14126,   449, 25259,   460, 20134,   461, 12549,   456, 16461,   500,\n",
      "          1427,  7214,   445,   456, 12483,   458, 12509,   385, 15805,  2238,\n",
      "          9589,   460, 14871,   500, 18298, 12495,   461,   384, 20344,   500,\n",
      "          1731,  8425,  3908, 14907,   458,   441,   456,     3],\n",
      "        [    2,  1042,  7405,  3177,     3,  1731,  8425,  7234, 14907,   464,\n",
      "         20344, 12505,   484,  1731,  8425,  7234,   484, 23297,   384, 20403,\n",
      "          5538,   464, 13700,  2386,   484, 12485,   385, 23771,  7819, 12488,\n",
      "         19638, 13160,   500, 19518,   384, 19638, 13160,  3107,   430, 25043,\n",
      "           464, 15895, 12496, 14580,   462, 13756,   461, 12675,   456, 12989,\n",
      "           458,   384,   395,   428,  7203,   384,  4141,   409,   491, 12494,\n",
      "           396,   458, 12737,   456, 27315, 12710,   500, 23771,  7819,   461,\n",
      "         15581, 14264,   456, 15163, 12727,   384, 19136, 12753,   464, 12829,\n",
      "           457, 12485,   461,   484, 15328,   444,   384,  1704,   460,   655,\n",
      "           500, 24396,   461, 14574, 18392, 12496,   384,  3619, 14217,   460,\n",
      "          6522,   484, 12485,   385, 24396,   430, 27956,  7172,   464,  1781,\n",
      "           461, 27080,   458,   441,   456, 12483,   464,   500, 14630,   457,\n",
      "         23365,   441, 12727,   384, 23771,  7819,   458,     3]]),\n",
      "                        'token_type_ids': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])},\n",
      " 'tokenized_questions': {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0]]),\n",
      "                         'input_ids': tensor([[    2,   395, 31113,  7105,   409, 13891,    85, 28822,   409,   396,\n",
      "           474, 12515, 15347,    16,   439,  7200,   384, 18833,   593,   593,\n",
      "           593,   458, 14656, 23050,   465,   384, 19311,   457,   742, 13315,\n",
      "         21657,    46,     3],\n",
      "        [    2, 13313, 13136,   397, 20344,   398,   464, 12593, 12784,   457,\n",
      "           384, 19136,   464, 26589,   465,  1042,  7405, 13037,   430,   384,\n",
      "         23771,  7819,   464, 26589,   465,   742, 21657,    46,     3,     0,\n",
      "             0,     0,     0]]),\n",
      "                         'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]])}}\n"
     ]
    }
   ],
   "source": [
    "pprint(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d6bcf8-2a7e-4748-8c67-6c09ffc8b833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c85a40c-f437-49cb-a506-6e7ea5bcd6c3",
   "metadata": {},
   "source": [
    "# BPRによるパッセージの埋め込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b92374-6265-430b-b1f9-ec11223376c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI王データセットのパッセージデータを読み込む\n",
    "passage_dataset = load_dataset(\"llm-book/aio-passages\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf812af-ef8d-4547-b30a-b0d0f54803dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'pageid', 'revid', 'text', 'section', 'title'],\n",
      "    num_rows: 4288198\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(passage_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "654c3dff-52fc-43e6-aa0f-0f628f0f0d28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(32768, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./outputs_bpr/passage_encoder/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "passage_encoder = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "device = 'cuda:0'\n",
    "passage_encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9307cad-9142-4265-ac0e-049c2601d613",
   "metadata": {},
   "source": [
    "### モデルによる埋め込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec293c6-f008-4848-af42-d0b36f47d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_passages(titles: list[str], texts: list[str]) -> np.ndarray:\n",
    "    \"\"\"BPRのパッセージエンコーダを用いてパッセージの埋め込みを計算\"\"\"\n",
    "    # パッセージにトークナイザを適用\n",
    "    tokenized_passages = tokenizer(\n",
    "        titles,\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=\"only_second\",\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    # トークナイズされたパッセージを実数埋め込みに変換\n",
    "    with torch.inference_mode():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            encoded_passages = passage_encoder(\n",
    "                **tokenized_passages\n",
    "            ).last_hidden_state[:, 0]\n",
    "\n",
    "    # 実数埋め込みをNumPyのarrayに変換\n",
    "    emb = encoded_passages.cpu().numpy()\n",
    "    # 0未満の値を0に、0以上の値を1に変換 -> バイナリに変換\n",
    "    emb = np.where(emb < 0, 0, 1).astype(bool)\n",
    "    # bool型からuint8型に変換\n",
    "    emb = np.packbits(emb).reshape(emb.shape[0], -1)\n",
    "    return emb\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd810ba8-5563-4bd3-9490-2c524afa2c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7f7114bb5b40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72035ea37ac4056b108b9d2f03c27ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4288198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# パッセージデータのすべての事例に埋め込みを行う\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m passage_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpassage_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membed_passages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3156\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3157\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3158\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3547\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3543\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3544\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3545\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3546\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3547\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3556\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3416\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3415\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3416\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3418\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3419\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3420\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# パッセージデータのすべての事例に埋め込みを行う\u001b[39;00m\n\u001b[1;32m      2\u001b[0m passage_dataset \u001b[38;5;241m=\u001b[39m passage_dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m example: {\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(\u001b[43membed_passages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m     },\n\u001b[1;32m      6\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36membed_passages\u001b[0;34m(titles, texts)\u001b[0m\n\u001b[1;32m     16\u001b[0m         encoded_passages \u001b[38;5;241m=\u001b[39m passage_encoder(\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenized_passages\n\u001b[1;32m     18\u001b[0m         )\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 実数埋め込みをNumPyのarrayに変換\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mencoded_passages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 0未満の値を0に、0以上の値を1に変換 -> バイナリに変換\u001b[39;00m\n\u001b[1;32m     23\u001b[0m emb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(emb \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# パッセージデータのすべての事例に埋め込みを行う\n",
    "passage_dataset = passage_dataset.map(\n",
    "    lambda example: {\n",
    "        \"embeddings\": list(embed_passages(example['title'], example['text']))\n",
    "    },\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739accf9-4551-4fef-9b8d-fcbd2cd9f39c",
   "metadata": {},
   "source": [
    "# 文書検索モデルとChatGPTを組み合わせる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "792e53e8-4f60-4c48-9dcf-77aa519dcdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5152446fe24752a00f420fba0b714c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166448b30ec64b009b1c02130c4aeaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/343M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09716f35b954ca8b37a02af6d5b3367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682a824385c04b49abf13a180a8c394f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/307M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42608cc6233343089cbb6e3337819661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/304M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b8fcf4b44d49049d85467ae36f842d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/296M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97a946881e34a48b0e6addc506b5db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/294M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cf2ea87b944a248380c73179076a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/289M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c945d1d3b1a416e9fda27819317922f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4288198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5259291275eb48f198aa9f0e5000bc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/634 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8596f7ddaea04dd3a8f186c70a6b2450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24306f3a82124fecb15c314f040f227a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e90e6fa8b564f039af4789009f8f22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/231k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a149c3f7c4274e6888d71d33c9dd8292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"llm-book/aio-passages-bpr-bert-base-japanese-v3\"\n",
    "passage_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "encoder_model_name = \"llm-book/bert-base-japanese-v3-bpr-question-aio\"\n",
    "encoder_pipeline = pipeline(\n",
    "    \"feature-extraction\", model=encoder_model_name, device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87af4a03-87fd-4a03-a50e-8d9beb2093f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4288198/4288198 [02:21<00:00, 30251.31it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_size = encoder_pipeline.model.config.hidden_size\n",
    "faiss_index = faiss.IndexBinaryIDMap2(faiss.IndexBinaryFlat(embed_size))\n",
    "\n",
    "with tqdm(total = len(passage_dataset)) as pbar:\n",
    "    i = 0\n",
    "    for batch in passage_dataset.iter(batch_size=512):\n",
    "        bs = len(batch['embeddings'])\n",
    "\n",
    "        # 埋め込みuint8に変換\n",
    "        batch_embeddings = np.array(batch['embeddings'], dtype=np.uint8)\n",
    "\n",
    "        batch_indices = np.arange(i, i + bs, dtype=np.int64)\n",
    "\n",
    "        # 埋め込みをインデックスに格納\n",
    "        faiss_index.add_with_ids(batch_embeddings, batch_indices)\n",
    "\n",
    "\n",
    "        pbar.update(n=bs)\n",
    "        i += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d275b552-d7ba-457f-8f7c-1f5a69ad9427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_questions(questions: list[str]) -> np.ndarray:\n",
    "    \"\"\"質問文を実数埋め込みに変換\"\"\"\n",
    "    output_tensor = encoder_pipeline(questions, return_tensors='pt')\n",
    "    embeddings = np.vstack([t.squeeze(0)[0] for t in output_tensor]) # clsの埋め込みのみ取得\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def binarize_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"実数埋め込みをバイナリ埋め込みに変換\"\"\"\n",
    "    binary_embeddings = np.where(embeddings < 0, 0, 1)\n",
    "    # uint8型に変換\n",
    "    packed_binary_embeddings = np.packbits(binary_embeddings, axis=-1)\n",
    "    return packed_binary_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e365f6a9-1fc1-456b-8967-7bd34ec1f7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "実数埋め込みのshape: (1, 768)\n",
      "バイナリ返還後のshape: (1, 96)\n"
     ]
    }
   ],
   "source": [
    "# 作成した関数のチェック\n",
    "q_embed = embed_questions([\"日本で一番高い山は何？\"])\n",
    "binary_q_embed = binarize_embeddings(q_embed)\n",
    "print(f'実数埋め込みのshape: {q_embed.shape}')\n",
    "print(f'バイナリ返還後のshape: {binary_q_embed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5f9ffca-3f9c-43f0-b73b-ecb3307b65cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['日本の領土に占める山間部の割合はおよそ7割程度である。2020年現在、日本で一番高い山は富士山で、逆に一番低い山は日和山である。日本は新期造山帯に位置し、多くの火山が見られる。日本では昔から人々が山と共生する文化が培われてきた。それらの山は里山とよばれ、農村などでは山を共有地として村全体で管理し、薪をとったり土や山菜などを利用する目的で利用され、手入れをされてきた。しかし、明治時代になるとそれらの山の中には国有地にされてしまったものも多く存在する。また、日本の山の中には霊峰とよばれ、民間信仰の場にされた山も多くある。山の中には昔から金山や銀山として、近代では銅や石炭などを入手するため多くの鉱山が作られ、鉱毒や粉塵などが問題になった。特に近代において、山林の多くで木材を得るための大規模な伐採などが行われた。', '3000 m峰は、独立峰の富士山と御嶽山及び飛騨山脈(北アルプス)と赤石山脈(南アルプス)の山域に限られている。24番目に高い山は剱岳 (2999 m) で、3000 mにわずか1 m届いていない。標高3000 mを越える一帯は森林限界のハイマツ帯で、高山植物の群生地となっている。また多くの3000 m峰のハイマツ帯は、ライチョウ(雷鳥)の生息地となっている。', '三角点は一般的に眺望の利く場所に設置され一等三角点は970点余りに上るが、その中で風格のある山容、優れた眺望、高い知名度、さらに概ね標高1,000m以上で登りがいのある山が選定された。一等三角点の最高峰は、標高3,121 mの南アルプスの赤石岳である。三角点がその山の最高峰とは限らず、山頂に三角点より高い場所があり、その地点すなわち標高点や測定点を以て山の高さとしている例も少なくない(例:早池峰、御嶽山、金剛山)。また連山を形成している場合、最高峰でないピークに一等三角点が設置されている例もある(例:赤城山、穂高岳、阿蘇山)。特殊な例としては雲仙普賢岳のように、噴火により一等三角点が最高峰でなくなった事例もある。利尻岳中腹の長官山のように全く山頂とは離れた地点に一等三角点が設置されている例もある(山頂は二等三角点「利尻絶頂」)。', 'この項では世界最高峰と考えられていた山(せかいさいこうほうとかんがえられていたやま)について述べる。19世紀初頭までアンデスが世界で最も高い山脈だと考えられていた。ヒマラヤの測量はインドに進出していたイギリス人によって19世紀初頭から行われた。8,000メートルを越える測量値も出たが、当初は専門家に信用されなかった。ヒマラヤが世界で一番高いと一般的に認められたのは、1830年代になってジョージ・エベレストが大三角測量を行ってからである。', '最高峰(さいこうほう)とは、ある大陸・島や地方、国などで一番高い山に対する呼称である。例えば世界の最高峰はエベレスト山(8848m)、日本の最高峰は富士山の剣ヶ峰(3776m)である。七大陸最高峰、各都道府県の最高峰など、テーマを決めて最高峰の完登に挑戦する登山者も少なくない。転じて、ある一群の中で最高のものの意にも使用される。']\n"
     ]
    }
   ],
   "source": [
    "scores,  passage_ids = faiss_index.search(binary_q_embed, k=5)\n",
    "print(passage_dataset[passage_ids[0]][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ff7fc7f-b657-44c9-a30a-570348d34ca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# faissのreconstructメソッドにidを渡すとそのpassageの埋め込みが取得できる\n",
    "passage_id = 0\n",
    "np.unpackbits(faiss_index.reconstruct(passage_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5e878b9b-79ca-4b73-8db8-cca0471022cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_passages(\n",
    "    questions: list[str],\n",
    "    binary_k: int = 2048,  # リランキングするための候補の取得数\n",
    "    top_k: int = 3,  # リランキング後に出力する最終的なパッセージ数\n",
    ") -> list[list[str]]:\n",
    "    \"\"\"質問から関連するパッセージを取得\"\"\"\n",
    "    # 質問をバイナリベクトルに変換する\n",
    "    q_embed = embed_questions(questions)\n",
    "    q_bin_embed = binarize_embeddings(q_embed)\n",
    "\n",
    "    # バイナリベクトルを使用して検索する -> 候補パッケージの取得\n",
    "    # output -> shape(question数, 選出されたids:512)\n",
    "    # 選出されたids:512は関連性が高い順に並んでいる => point(1)\n",
    "    binary_k_scores, binary_k_p_ids = faiss_index.search(\n",
    "        q_bin_embed, binary_k\n",
    "    )\n",
    "\n",
    "    batch_size = len(questions)\n",
    "    embed_dim = q_bin_embed.shape[-1] * 8 # byte変換を行っているので＊８をおこなって、bit基準にしている\n",
    "    # インデックスからパッセージのベクトルを復元\n",
    "    p_uint8_embed = [\n",
    "        faiss_index.reconstruct(int(p_id))\n",
    "        for p_id in binary_k_p_ids.flatten() # 1次元にreshape\n",
    "    ]\n",
    "    # uint8からboolに変換\n",
    "    p_bin_embed = np.vstack([np.unpackbits(e) for e in p_uint8_embed])\n",
    "    # 型とシェイプを変換\n",
    "    p_bin_embed = p_bin_embed.astype(np.float32).reshape(\n",
    "        batch_size, binary_k, embed_dim\n",
    "    )\n",
    "    p_bin_embed = p_bin_embed * 2 - 1 # 0, 1のバイナリ値を-1, 1のバイナリに変換している\n",
    "\n",
    "    # 質問の実数ベクトルとパッセージのバイナリベクトルで再度スコアを計算する\n",
    "    re_scores = np.einsum(\"ijk,ik->ij\", p_bin_embed, q_embed)\n",
    "    top_k_indices = np.argsort(-re_scores, axis=-1)[:, :top_k] \n",
    "    top_k_p_ids = np.take_along_axis(\n",
    "        binary_k_p_ids, top_k_indices, axis=-1\n",
    "    )\n",
    "\n",
    "    # top_kのテキストを整形して出力する\n",
    "    retrieved_texts: list[list[str]] = []\n",
    "    for p_ids in top_k_p_ids:\n",
    "        formatted_texts = [\n",
    "            f\"タイトル：{passage_dataset[i]['title']}\\n\"\n",
    "            f\"本文：{passage_dataset[i]['text']}\"\n",
    "            for i in p_ids.tolist()\n",
    "        ]\n",
    "        retrieved_texts.append(formatted_texts)\n",
    "\n",
    "    return retrieved_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9bf452de-e43a-458a-aeda-0c15e4997c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タイトル：日本の山\n",
      "本文：日本の領土に占める山間部の割合はおよそ7割程度である。2020年現在、日本で一番高い山は富士山で、逆に一番低い山は日和山である。日本は新期造山帯に位置し、多くの火山が見られる。日本では昔から人々が山と共生する文化が培われてきた。それらの山は里山とよばれ、農村などでは山を共有地として村全体で管理し、薪をとったり土や山菜などを利用する目的で利用され、手入れをされてきた。しかし、明治時代になるとそれらの山の中には国有地にされてしまったものも多く存在する。また、日本の山の中には霊峰とよばれ、民間信仰の場にされた山も多くある。山の中には昔から金山や銀山として、近代では銅や石炭などを入手するため多くの鉱山が作られ、鉱毒や粉塵などが問題になった。特に近代において、山林の多くで木材を得るための大規模な伐採などが行われた。\n",
      "タイトル：日本の山一覧 (3000m峰)\n",
      "本文：3000 m峰は、独立峰の富士山と御嶽山及び飛騨山脈(北アルプス)と赤石山脈(南アルプス)の山域に限られている。24番目に高い山は剱岳 (2999 m) で、3000 mにわずか1 m届いていない。標高3000 mを越える一帯は森林限界のハイマツ帯で、高山植物の群生地となっている。また多くの3000 m峰のハイマツ帯は、ライチョウ(雷鳥)の生息地となっている。\n",
      "タイトル：一等三角点百名山\n",
      "本文：三角点は一般的に眺望の利く場所に設置され一等三角点は970点余りに上るが、その中で風格のある山容、優れた眺望、高い知名度、さらに概ね標高1,000m以上で登りがいのある山が選定された。一等三角点の最高峰は、標高3,121 mの南アルプスの赤石岳である。三角点がその山の最高峰とは限らず、山頂に三角点より高い場所があり、その地点すなわち標高点や測定点を以て山の高さとしている例も少なくない(例:早池峰、御嶽山、金剛山)。また連山を形成している場合、最高峰でないピークに一等三角点が設置されている例もある(例:赤城山、穂高岳、阿蘇山)。特殊な例としては雲仙普賢岳のように、噴火により一等三角点が最高峰でなくなった事例もある。利尻岳中腹の長官山のように全く山頂とは離れた地点に一等三角点が設置されている例もある(山頂は二等三角点「利尻絶頂」)。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "passages = retrieve_passages([\"日本で一番高い山は何？\"], top_k=3)[0]\n",
    "for passage in passages:\n",
    "    print(passage)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6547eb4-5eac-450a-9d7e-03b58c312855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733921de-1907-4a0b-909b-569312521e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58074d5-1a69-484c-8f73-83da1e9d22cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c48c9-cd81-42de-a16b-bc7abc5fd73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489b48c-c0e4-4a29-bbc9-886ef6d3ef10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef5928-4778-45d4-915f-f136852ec3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "21d17ac6-28bf-47de-aab9-4c5501c8a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_embed = embed_questions([\"日本で一番高い山は何？\", \"世界で一番高い山は？\"])\n",
    "q_bin_embed = binarize_embeddings(q_embed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "07aa5042-94bb-4553-ae67-15c280ae397a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 96)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_bin_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "27a66244-261b-48fe-8715-84da4029def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_k_scores, binary_k_p_ids = faiss_index.search(\n",
    "    q_bin_embed, 512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f5f8930-8b2f-4825-a287-665edec9b461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 512)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_k_p_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "73d3489c-6839-42c1-935d-0222592fc9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_k_p_ids.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "568c88c8-c57f-4bf3-874f-45237bb5a106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 5]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "re_scores = np.array([\n",
    "    [1, 2, 3, 4, 5, 6, 7],\n",
    "    [7, 6, 5, 4, 3, 2, 1]\n",
    "])\n",
    "\n",
    "top_k = 2\n",
    "\n",
    "top_k_indices = np.argsort(-re_scores, axis=-1)[:, :top_k]\n",
    "print(top_k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be44738-abea-442a-aa7a-00874e95bb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2308d-24d2-4ddb-b673-ba9e05dea158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10667854-1d12-4b9e-81fc-ee5071a01c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d63ff7-059f-4184-acbf-a15e65e226ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df410024-8566-4d05-a0ff-23e49c04391e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c60db78-efee-4d94-9009-15d6a6d80537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a62e842-e4e2-45c8-ae5d-1321f8f6ebe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877e6c9-cc4f-4af6-860f-ba2e8edda145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
