{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e3659e0-83b8-4584-a55e-72eff8ecdc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import logging\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ba7f8c9-0b61-4cde-bc9d-3d9301fb5137",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '（' (U+FF08) (2627790753.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[27], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ログレベルを設定（この例ではWARNINGレベル以上のみ表示）\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '（' (U+FF08)\n"
     ]
    }
   ],
   "source": [
    " ログレベルを設定（この例ではWARNINGレベル以上のみ表示）\n",
    "logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04d830bc-f03c-4c50-afd7-d995a7e31031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-marc_ja/snapshots/7b47edf80477fd9da0ee1bc1908326ac012d624f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"llm-book/bert-base-japanese-v3-marc_ja\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-marc_ja/snapshots/7b47edf80477fd9da0ee1bc1908326ac012d624f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"llm-book/bert-base-japanese-v3-marc_ja\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-marc_ja/snapshots/7b47edf80477fd9da0ee1bc1908326ac012d624f/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at llm-book/bert-base-japanese-v3-marc_ja.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-marc_ja/snapshots/7b47edf80477fd9da0ee1bc1908326ac012d624f/vocab.txt\n",
      "loading file spiece.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-marc_ja/snapshots/7b47edf80477fd9da0ee1bc1908326ac012d624f/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-marc_ja/snapshots/7b47edf80477fd9da0ee1bc1908326ac012d624f/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n"
     ]
    }
   ],
   "source": [
    "# llm-book/bert-base-japanes-v3-marc_jaを使用する\n",
    "text_classification_pipline = pipeline(\n",
    "    model=\"llm-book/bert-base-japanese-v3-marc_ja\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536da4dc-2a80-42ba-aa6b-cc55a75319c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'positive', 'score': 0.9993619322776794}\n"
     ]
    }
   ],
   "source": [
    "# positiveの予想\n",
    "positive_text = \"世界には言葉がわからなくても感動する音楽がある。\"\n",
    "print(text_classification_pipline(positive_text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efdfe017-024c-499f-8292-54432b0a0cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'positive', 'score': 0.9981855750083923}\n"
     ]
    }
   ],
   "source": [
    "# negative\n",
    "\n",
    "nagative_text = 'アンニュイ'\n",
    "print(text_classification_pipline(nagative_text)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1fc47-18ad-40e6-8c83-5e8fc3cb5a0d",
   "metadata": {},
   "source": [
    "## 自然言語推論(NLI)  \n",
    "\n",
    "・二つのテキストの論理関係を予測するタスク  \n",
    "・言語モデルの意味理解力を評価するために使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f9967eb-be99-475e-b92f-ab1da7ba68ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jnli/snapshots/9056fce079ed3fc284c9b2d1c2abccae3d13af61/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"llm-book/bert-base-japanese-v3-jnli\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"contradiction\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 1,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jnli/snapshots/9056fce079ed3fc284c9b2d1c2abccae3d13af61/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"llm-book/bert-base-japanese-v3-jnli\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"contradiction\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 1,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jnli/snapshots/9056fce079ed3fc284c9b2d1c2abccae3d13af61/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at llm-book/bert-base-japanese-v3-jnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jnli/snapshots/9056fce079ed3fc284c9b2d1c2abccae3d13af61/vocab.txt\n",
      "loading file spiece.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jnli/snapshots/9056fce079ed3fc284c9b2d1c2abccae3d13af61/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jnli/snapshots/9056fce079ed3fc284c9b2d1c2abccae3d13af61/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'entailment', 'score': 0.9964311122894287}\n"
     ]
    }
   ],
   "source": [
    "nli_pipeline = pipeline(model=\"llm-book/bert-base-japanese-v3-jnli\", device='cuda:0')\n",
    "text = \"二人の男性がジェット機を見ています\"\n",
    "entailment_text = \"ジェット機を見ている人が二人います\"\n",
    "\n",
    "print(nli_pipeline({'text':text, 'text_pair':entailment_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5305713-e22d-4348-92d2-e9eedc40b3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'contradiction', 'score': 0.9990535378456116}\n"
     ]
    }
   ],
   "source": [
    "contradiction_text = \"二人の男性が飛んでいます\"\n",
    "# textとcontradiction_textの論理関係を予測\n",
    "print(nli_pipeline({\"text\": text, \"text_pair\": contradiction_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "811e3ded-3f8e-4974-9b07-7a9b7eeadb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'neutral', 'score': 0.9579205513000488}\n"
     ]
    }
   ],
   "source": [
    "netral_text = '二人の男性が、白い飛行機を眺めています'\n",
    "print(nli_pipeline({'text':text, 'text_pair':netral_text}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7fcfc-f3b4-4fb6-a4d4-631564191d2a",
   "metadata": {},
   "source": [
    "## 意味的類似度の計算  \n",
    "  \n",
    "・二つのテキストの意味が似ている度合いをスコアとしてよそくするタスク。  \n",
    "・情報検索や、複数テキストの内容の整合性を確認する際に役立ちます。  \n",
    "・0~5の値が返る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49843049-3d71-44fc-b145-879f1eb341fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5703563690185547\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "text_sim_pipeline = pipeline(\n",
    "    model=\"llm-book/bert-base-japanese-v3-jsts\",\n",
    "    function_to_apply=\"none\",\n",
    ")\n",
    "text = \"川べりでサーフボードを持った人たちがいます\"\n",
    "sim_text = \"サーファーたちが川べりに立っています\"\n",
    "# textとsim_textの類似度を計算\n",
    "result = text_sim_pipeline({\"text\": text, \"text_pair\": sim_text})\n",
    "print(result[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ff89253-6a43-4176-8d54-be4f11072b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041621580719947815\n"
     ]
    }
   ],
   "source": [
    "dissim_text = \"トイレの壁に黒いタオルがかけられています\"\n",
    "# textとdissim_textの類似度を計算\n",
    "result = text_sim_pipeline({\"text\": text, \"text_pair\": dissim_text})\n",
    "print(result[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbf1f0a2-58af-452d-848a-2509cb196e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296f381e821542f580e4f5e24767af7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/634 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09915adfe37e46d9a5d9c777a983b7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93a14ab1f624fab9f7ec801347d2c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233e02879b1241c0a4cea7c8975d95f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/231k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650db659853e48c68a0381f83042ca74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# コサイン類似度を使用した意味的類似度\n",
    "sim_enc_pipeline = pipeline(\n",
    "    model=\"llm-book/bert-base-japanese-v3-unsup-simcse-jawiki\",\n",
    "    task=\"feature-extraction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cb4b4c3-e777-455f-a6bb-00612f5efcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8568588495254517\n"
     ]
    }
   ],
   "source": [
    "# ベクトルを獲得\n",
    "text_emb = sim_enc_pipeline(text, return_tensors=True)[0][0]\n",
    "sim_emb = sim_enc_pipeline(sim_text, return_tensors=True)[0][0]\n",
    "\n",
    "# コサイン類似度\n",
    "sim_pair_score = cosine_similarity(text_emb, sim_emb, dim=0)\n",
    "print(sim_pair_score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1be1b5fb-613e-467c-997b-dd9e710395ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45887044072151184\n"
     ]
    }
   ],
   "source": [
    "# dissim_textのベクトルを獲得\n",
    "dissim_emb = sim_enc_pipeline(dissim_text, return_tensors=True)[0][0]\n",
    "# textとdissim_textの類似度を計算\n",
    "dissim_pair_score = cosine_similarity(text_emb, dissim_emb, dim=0)\n",
    "print(dissim_pair_score.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab027d30-1ab8-4ad6-a2f6-3bf47cd08143",
   "metadata": {},
   "source": [
    "## 固有表現認識（NER）  \n",
    "・テキストに含まれる固有表現を抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c14df833-f74c-4ebd-9f69-91c2d5f1e9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba87f5d768da40d6aee37f01b155d239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f635dc612b3a4a7192948df6fcf5cda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff971ae07e74b3381370266433720a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8cd6d2a71e426eb134b15de3f4e076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/231k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad7931baaa24800bf1f9385fdbcee8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': None,\n",
      "  'entity_group': '人名',\n",
      "  'score': 0.99823624,\n",
      "  'start': None,\n",
      "  'word': '大谷 翔平'},\n",
      " {'end': None,\n",
      "  'entity_group': '地名',\n",
      "  'score': 0.9986874,\n",
      "  'start': None,\n",
      "  'word': '岩手 県 水沢 市'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    model=\"llm-book/bert-base-japanese-v3-ner-wikipedia-dataset\",\n",
    "    aggregation_strategy=\"simple\",\n",
    ")\n",
    "text = \"大谷翔平は岩手県水沢市出身のプロ野球選手\"\n",
    "# text中の固有表現を抽出\n",
    "pprint(ner_pipeline(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be159874-7ef2-4d1e-8db5-04a5ee2713a6",
   "metadata": {},
   "source": [
    "## 要約生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "741add9a-4b0f-444a-9031-7a3e761a358a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e3ad2f0d9e4312bd5ed2836aa39153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/768 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8da50ed5054f6097796665dda69a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf44a35369341e29b36b95706975f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248ab5d300af428b842c17beead7bdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed191eed39d34a55aafdc2c688c02fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e741ca7128e7429093a0b4ba67a50c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb248a9fbfa4ec08abf2b1d316ece68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今夜はNHKスペシャル「世界を変えた男 スティーブ・ジョブズ」をチェック!\n"
     ]
    }
   ],
   "source": [
    "text2text_pipeline = pipeline(\n",
    "    model=\"llm-book/t5-base-long-livedoor-news-corpus\"\n",
    ")\n",
    "article = \"ついに始まった３連休。テレビを見ながら過ごしている人も多いのではないだろうか？　今夜オススメなのは何と言っても、NHKスペシャル「世界を変えた男 スティーブ・ジョブズ」だ。実は知らない人も多いジョブズ氏の養子に出された生い立ちや、アップル社から一時追放されるなどの経験。そして、彼が追い求めた理想の未来とはなんだったのか、ファンならずとも気になる内容になっている。 今年、亡くなったジョブズ氏の伝記は日本でもベストセラーになっている。今後もアップル製品だけでなく、世界でのジョブズ氏の影響は大きいだろうと想像される。ジョブズ氏のことをあまり知らないという人もこの機会にぜひチェックしてみよう。 世界を変えた男　スティーブ・ジョブズ（NHKスペシャル）\"\n",
    "# articleの要約を生成\n",
    "print(text2text_pipeline(article)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eff85211-ca18-4385-95d6-e6b3cf465e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c256313b7c9f4a5db6a9ee7ee84f4001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/282 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e30d19a8364e4cb3acec32583689e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/784k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db422df2bbbf44bda4bc3c38c5d09f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/153 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['▁', '今日', 'は', '天気', 'が良い', 'の', 'で']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizerのロード\n",
    "tokenizer = AutoTokenizer.from_pretrained('abeja/gpt2-large-japanese')\n",
    "\n",
    "# テキストのトークン化\n",
    "tokenizer.tokenize('今日は天気が良いので')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d10c4ea-f058-4155-a32b-f8a1f7914308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は天気が良いので外でお弁当を食べました。\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('abeja/gpt2-large-japanese')\n",
    "\n",
    "# トークン化 -> ベクトル化\n",
    "input = tokenizer('今日は天気が良いので', return_tensors='pt')\n",
    "\n",
    "outputs = model.generate(\n",
    "**input,\n",
    "    max_length=15,\n",
    "    pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "generated_text = tokenizer.decode(\n",
    "    outputs[0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9961baa-fdb9-425d-b9d1-46b1c4a086c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    7,  4639,    15, 17110, 16958,    10,    20,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1b1da-de4b-40af-84f0-a85bce6fd4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f05e7-e973-464c-990d-a2fd12ec411e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
